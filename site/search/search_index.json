{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Alleviate Children's Health Issues through Games and Machine Learning This interactive book is a summary of my thesis research project carried out at completion of my Undergraduate Degree in Electronic Engineering at the University of Southampton. A summary of this work is available as an article published for Towards Data Science and part of the code is available in this GitHub repository . I would like to thank my supervisor Professor Koushik Maharatna, for giving me the opportunity to do this project. This experience enabled me broaden my knowledge on a variety of topics and skills which will play a crucial role in my upcoming career. I would additionally like to thank, Rafael Gutierrez Nuno and Noura Alotaibi for having introduced me to the Enobio 20 EEG Cap system and to the data-set used throughout this project. Finally, I express my gratitude to my second examiner, Professor Martin Charlton, for his support throughout the project. \"The reasonable man adapts himself to the world: the unreasonable one persists in trying to adapt the world to himself. Therefore all progress depends on the unreasonable man.\" \u200b - George Bernard Shaw, Man and Superman","title":"Home"},{"location":"#alleviate-childrens-health-issues-through-games-and-machine-learning","text":"This interactive book is a summary of my thesis research project carried out at completion of my Undergraduate Degree in Electronic Engineering at the University of Southampton. A summary of this work is available as an article published for Towards Data Science and part of the code is available in this GitHub repository . I would like to thank my supervisor Professor Koushik Maharatna, for giving me the opportunity to do this project. This experience enabled me broaden my knowledge on a variety of topics and skills which will play a crucial role in my upcoming career. I would additionally like to thank, Rafael Gutierrez Nuno and Noura Alotaibi for having introduced me to the Enobio 20 EEG Cap system and to the data-set used throughout this project. Finally, I express my gratitude to my second examiner, Professor Martin Charlton, for his support throughout the project. \"The reasonable man adapts himself to the world: the unreasonable one persists in trying to adapt the world to himself. Therefore all progress depends on the unreasonable man.\" \u200b - George Bernard Shaw, Man and Superman","title":"Alleviate Children's Health Issues through Games and Machine Learning"},{"location":"abstract/","text":"Abstract Children can develop disabilities at birth. Identifying their problems at an early stage can lead to more consistent improvements later in life. This is mainly dependent on neuroplasticity (the brain's ability to reorganise itself throughout life), which is much higher during the first few years of our life. Children may be affected by different types/forms of disorders like disability and autism. These can be caused by factors such as: genetics, environment, diet and lifestyle. Therefore, prevention can play a crucial role in decreasing the number of children's negatively affected. Different intervention approaches such as The Early Start Denver Model (ESDM) and Applied Behaviour Analysis (ABA) can be used to aid improvements. Using both software and hardware solutions can enhance progresses increasing children's level of engagement.","title":"Abstract"},{"location":"abstract/#abstract","text":"Children can develop disabilities at birth. Identifying their problems at an early stage can lead to more consistent improvements later in life. This is mainly dependent on neuroplasticity (the brain's ability to reorganise itself throughout life), which is much higher during the first few years of our life. Children may be affected by different types/forms of disorders like disability and autism. These can be caused by factors such as: genetics, environment, diet and lifestyle. Therefore, prevention can play a crucial role in decreasing the number of children's negatively affected. Different intervention approaches such as The Early Start Denver Model (ESDM) and Applied Behaviour Analysis (ABA) can be used to aid improvements. Using both software and hardware solutions can enhance progresses increasing children's level of engagement.","title":"Abstract"},{"location":"chapter1/","text":"Introduction Motivation Autism Spectrum Disorder (ASD) is a type of neuro-developmental disorder. These types of disorders are mainly considered as disruption of regular brain functioning. Over the last few years there has been reported an increased number of children developing this pathology. Data drawn from a 2014 survey in the United States has shown an increase of 16 percent compared to the previous same survey conducted in 2012. According to this research, about 1 in 59 children in the United States have Autism Spectrum Disorder (\u201cCDC\u2019s Autism and Developmental Disabilities Monitoring (ADDM) Network\u201d, [1]). Recent research have shown that intensive early start intervention approaches can lead to major improvements to this condition. Although early intervention can be used with all the children on the ASD spectrum, response to these treatments can vary substantially from child to child. There can be many different reasons to cause diversity in improvements such as: symptom severity, joint attention, adaptive skills, interest in objects, imitation and play skills [2]. Objectives Current strategies used to alleviate autism conditions relies on three interdependent steps: Early detection. Approximately 25 hours per week of intensive personalised intervention. An effective monitoring of a child\u2019s progress. This project aims, firstly to outline the main behavioural intervention techniques used nowadays such as the The Early Start Denver Model (ESDM) and Applied Behaviour Analysis (ABA) and to then offer a suite of multi-player games as personalised delivery intervention for the children. These type of games are designed to be accessible both in a clinical and a home environment to facilitate intervention. In order to record the brain activity of children while playing the games, EEG cap readings are synchronised with the game and results are stored in a CSV file. These recordings will play a crucial role in observing if the game is stimulating the child and which areas are stimulated most in particular. Using this information and keeping track of the child\u2019s game scores, it will then be possible to examine what kind of improvements have been made during a predetermined time period. Analysing these results will also serve as feedback to understand what type of games can lead to most substantial improvements over others. Finally, the use of Machine Learning can facilitate early detection of disabilities, making use of available data to find patterns to distinguish between children suffering the disorder and those not.","title":"Introduction"},{"location":"chapter1/#introduction","text":"","title":"Introduction"},{"location":"chapter1/#motivation","text":"Autism Spectrum Disorder (ASD) is a type of neuro-developmental disorder. These types of disorders are mainly considered as disruption of regular brain functioning. Over the last few years there has been reported an increased number of children developing this pathology. Data drawn from a 2014 survey in the United States has shown an increase of 16 percent compared to the previous same survey conducted in 2012. According to this research, about 1 in 59 children in the United States have Autism Spectrum Disorder (\u201cCDC\u2019s Autism and Developmental Disabilities Monitoring (ADDM) Network\u201d, [1]). Recent research have shown that intensive early start intervention approaches can lead to major improvements to this condition. Although early intervention can be used with all the children on the ASD spectrum, response to these treatments can vary substantially from child to child. There can be many different reasons to cause diversity in improvements such as: symptom severity, joint attention, adaptive skills, interest in objects, imitation and play skills [2].","title":"Motivation"},{"location":"chapter1/#objectives","text":"Current strategies used to alleviate autism conditions relies on three interdependent steps: Early detection. Approximately 25 hours per week of intensive personalised intervention. An effective monitoring of a child\u2019s progress. This project aims, firstly to outline the main behavioural intervention techniques used nowadays such as the The Early Start Denver Model (ESDM) and Applied Behaviour Analysis (ABA) and to then offer a suite of multi-player games as personalised delivery intervention for the children. These type of games are designed to be accessible both in a clinical and a home environment to facilitate intervention. In order to record the brain activity of children while playing the games, EEG cap readings are synchronised with the game and results are stored in a CSV file. These recordings will play a crucial role in observing if the game is stimulating the child and which areas are stimulated most in particular. Using this information and keeping track of the child\u2019s game scores, it will then be possible to examine what kind of improvements have been made during a predetermined time period. Analysing these results will also serve as feedback to understand what type of games can lead to most substantial improvements over others. Finally, the use of Machine Learning can facilitate early detection of disabilities, making use of available data to find patterns to distinguish between children suffering the disorder and those not.","title":"Objectives"},{"location":"chapter2/","text":"Background Theory Autism Spectrum Disorders (ASD) Autism Spectrum Disorder is used as an umbrella term to enclose different types of disabilities. Suffering from ASD can lead to many types of difficulties, which can vary vastly between patients, such as communication, social, language and motor disorders. Different types of Autism Spectrum Disorders include: Asperger Syndrome = children affected can have an high intelligence quotient (IQ) , but suffer of social interaction problems. Autistic Disorder = children affected demonstrate unusual behaviours, language delays and communication difficulties. Pervasive developmental disorder, not otherwise specified (PDD-NOS) = children who display some ASD symptoms but do not meet all the Asperger Syndrome or Autistic Disorders criteria. Genes can play an important role in determining if a child is affected or not by autism, but there is still a lot that can be done to prevent it. Mothers who suffer a low iron intake, have an unhealthy lifestyle or are exposed to a high level of air pollution are in fact more likely to give birth to autistic children. Early signs to understand if a child is affected or not by autism can be lack of: happy expressions, response to stimulus, gesturing for communicating and attempts to speak [3] (Figure 2.1). Figure 2.1: Autism Symptoms. Image reproduced from: [4] Many behavioural therapies have been designed during the last few decades in order to aid children affected by autism such as: Floortime, Pivotal Response Treatment (PRT), Verbal Behavior Therapy, ABA and ESDM. During Floortime, parents and therapists take part in games the child likes to play, encouraging the child to take initiatives on how to develop the game and share this activity with other people. In PRT the therapist aims instead to stimulate specific vital developmental areas such as self-management and response to cues. Verbal Behavior Therapy was designed to help a child improve its language and communicative abilities. This is done by making children aware of the importance and benefits of using words as a way to express their needs and desires. Nowadays, there does not exist a recognised medical test for autism diagnostic. Cases are examined individually by doctors for classification. Online screening tools such as Q-Chat are currently available to help parents understand if their child is affected or not by autism [5]. Uses of Machine Learning (to analyse patients EEG readings) and Computer Vision (to detect, from video recording, behavioural and communication impairments) might provide a useful solution to this problem. Applied Behavior Analysis (ABA) Applied Behavior Analysis is a therapy approach based on finding out how learning takes place and how behaviour can be affected by the surrounding environment. It's main objective is to encourage behaviours that are positive for the patient and discourage behaviours that can negatively affect learning or have detrimental effects on the patient. One of the main parts ABA is interested in, is to understand what happens before and after the occurrence of a behaviour. ABA therapy can lead to an increase in social, communication skills and attention. This type of intervention approach can be used not only to help children (and potentially also adults) with autism but also other types of physical behaviour disorders such as eating disorders. ABA efficacy has been successfully proven thanks to many years of studies accomplished by acclaimed institutions and researchers [6]. Early Start Denver Model (ESDM) The Early Start Denver Model is a teaching play based behaviour therapy used to help children with disabilities. This approach is based on ABA, and is aimed at children between 1 to 4 years old. The main objective of ESDM is to help children improve their communication, language and social skills. This is achieved through games and joint activities. ESDM is typically used in both a domestic and clinical setting (additionally also in schools). In this way, both therapist and parents get personally involved in the therapy. Conducting part of the therapy at home (between children and parents) can also lead to substantial economical savings because it consistently reduces the number of weekly hospital hours of treatment needed (traditionally about 25 hours per week). Additionally, thanks to ESDM, children can also achieve better results because they might feel more comfortable to play at home with their parents rather than with a doctor in a hospital [7]. ESDM approach relies on one to one sessions between therapist and child and personalised intervention tailored according to the needs and improvements of the child. Studies have shown that ESDM can lead to more substantial improvements in children's IQ and adaptive behaviour compared to typical community-intervention approaches [8]. Electroencephalogram (EEG) EEG uses electrodes (small metal disks) placed on the scalp of the patient to track brainwave patterns and send the results to a computer for later analysis. Brain waves are observed measuring the electrical activity in the brain (electrical pulses of neurons communicating each other). EEG is generally used to identify occurrences such as changes of behaviour and seizures. Brainwaves can be divided in different bands according to their travelling speed (Hertz) [9]: Infra-Low: less than 0.5 Hz = basic cortical rythm Delta Waves: 0.5 to 3 Hz = dreamless sleep Theta Waves: 3 to 8 Hz = sleep and meditation Alpha Waves: 8 to 12 Hz = brain resting, flow of thoughts Beta Waves: 12 to 38 Hz = standard consciousness waking state Gamma Waves: 38 to 42 Hz = love, altruism etc... Another form of EEG analysis is quantitative EEG (qEEG). QEEG aims to analyse brainwaves using statistic and mathematics. Thanks to a process of quantitative comparison and accurate measurements, qEEG is able to recreate a map of the brain. A qEEG application example can be a research carried out by Lineu C. Fonseca about \"Quantitative EEG in children with learning disabilities\" [10], which has shown that children who suffer with reading and/or writing disabilities had higher delta activity in frontal-temporal regions than average. Serious Games and Gamification One of the aims of this project was to design a suite of games to help children with disabilities. Games created for a purpose other than entertainment are notoriously known as Serious Games. While playing a Serious Game, players will be likely asked to solve problems. Instead, traditional games mainly focus on action. In order to make learning effective, the main component required is engagement with the subject. Different mediums for learning are nowadays used such as: reading books, articles, manuals and educational videos. Each of these mediums have their advantages and disadvantages; the strength of delivering educational content through a game-based environment is the increase of student's engagement. What contributes to that are factors such as: the presence of a story-line, challenge and competition against themselves and others to improve their game scores, taking risky decision and paying the consequences for their actions, and learning to follow and understand rules. These concepts have applications in both games and in the real world, users makes mistakes and learn from them to make improvements. Serious Games can have a wide ambit of application as: health-care, defence, scientific and educational purpose's. Multinational companies have made use of Serious Games during the last few years. Examples include: \"Microsoft Flight Simulator\", \"IBM city one\" and \"Amnesty the Game\" [11]. Another application of game mechanics in non-gaming contexts is Gamification. In ordinary use, the terms Serious Games and Gamification get confused with each-other. Gamification aims to examine what engages most its potential players and then builds a game based on the gathered data. Designing games suited for each individual according to their needs and abilities can lead to positive behavioural improvements and increase player motivation. In contrast, Serious Games take business concepts and attempts to apply them using gaming techniques. Serious Games are not generally used as an exclusive learning technique but to reinforce understanding [12]. Figure 2.2: Serious gaming applications growth in academia and industry - Image reproduced from [13] - \"An overview of Serious Games\" - Fedwa Laamarti et al. Artificial Intelligence (AI) The birth of AI goes back to the end of the Second World War, starting with Alan Turing. At that point in time, German forces were using the Enigma Machine to encrypt their messages. Turing, using the Bombe machine, successfully managed to crack the 'Enigma' code. These two machines (Enigma and Bombe) constituted the foundations of Machine Learning. In fact, according to the Turing Test, a machine could be considered as intelligent if it is able to converse with humans without letting them find out it's true nature (therefore winning the \"imitation game\") [14]. Today, Artificial Intelligence has undergone impressive advancements. AI can be subdivided into three different levels according to the ability of machines to perform intellectual tasks logically and independently: Narrow AI = machines are more efficient than humans in performing very specific tasks (but not trying to perform other types of tasks). General AI = machines are as intelligent as human beings. Strong AI = machines perform better than humans in different ambit (in tasks that we might or not be able to perform at all). Right now we are only at a Narrow AI level. Artificial Intelligence can be broken down in many different categories such as: Machine Learning, Natural Language Processing (NLP), Vision, Robotics and Autonomous Vehicles [15] (Figure 2.3). Figure 2.3: Different AI fields. Image reproduced from: [15] One of the most frequently used AI tools is Machine Learning. There are three main types of machine learning algorithms used: Supervised Learning = using a labelled training set to train a model, to then make predictions on unlabelled data. Unsupervised Learning = giving a model an unlabelled data-set, the model has then to try to find patterns in the data to make predictions. Reinforcement Learning = training a model trough a reward mechanism to encourage positive behaviours in case of good performance (particularly used in robotics applications). The most important sub-field of Machine Learning is Deep Learning (DL). DL is mainly inspired by the structure and function of the human brain and therefore makes use of artificial neural networks. One of the first attempts in this field was the perceptron (which had very limited performance). Successive development of deep neural networks, adding multiple hidden units, using non linear activation functions and back-propagation algorithms lead to far greater results. Figure 2.4: AI, ML and DL relation. Image reproduced from: [16] Long Short-Term Memory (LSTM) LSTM is a type of Recurrent Neural Network (RNN). RNN were initially developed to make up for the inability of traditional neural networks to work with time series. This is because traditional neural networks, in contrast with RNN, lack of a memory mechanism to remember its past input values (all the inputs are fed in at once and information moves only in one direction; forward). RNN are able to overcome these shortfalls keeping information in a loop, therefore enabling the network to make decisions using not only the current input but also the previous ones (input are added sequentially, one input is added after the previous one has been computed). Therefore, to some extent, a RNN can be considered as a collection of artificial neural networks passing each output from the previous one. Figure 2.5: Artificial Neural Network equivalent of RNN Cell. Image reproduced from [17] RNN are although still far from being perfect. They are in fact just able to retain short term information and might suffer from vanishing/exploding gradients when performing the backwards pass. In order to solve these kinds of problem, the LSTM architecture was created. LSTMs are used to tackle time series forecasting, speech recognition and music generation tasks. The difference between an RNN and an LSTM lies in the repeating module (A in Figure 2.5). LSTM makes use of a far more structured architecture (Figure 2.6). Each module has three inputs: the current time step, an input from the previous module and the memory from the previous module. This way, each module can alter or preserve the network memory. Figure 2.6: LSTM Module. Image reproduced from: [18] Each LSTM module can be considered to be formed by three gates: input (I t ) , forget (f t ) and output gate (o t ). The input gate decides if a piece of information is important enough or not to be remembered (Equation 2.1). The forget gate (Equation 2.2) decides if a piece of information stored is still relevant or not (and therefore has to be deleted). The output gate determines if a particular information has to have or not a weight in the current time step (Equation 2.3). The Sigmoid function (\u03c3, Equation 2.4) is used to squish the output between any value from zero (making the gate block everything) to one (making the gate pass through everything). A neuron gate weight (w x ) represents the strength of the connection, while the bias (b x ) is used shift the activation function to fit best the data. Convolutional Neural Network (CNN) CNN are a class of neural networks typically used for image recognition/classification, but can be also used for time series analysis. This can be done by converting the time series in a grayscale image like format. The network takes as input a three-dimensional array (Height \u00d7 Width \u00d7 Depth) representing an image. A CNN is composed in two main sections: a feature extraction part and a classification part. During the feature extraction part, convolutions and padding layers are applied to extract the main characteristics embedded in the input vector. In the classification part, fully connected layers (converts our three-dimensional input into a one-dimensional array) are then used applying a Softmax function to classify the detected features. Figure 2.7: CNN structure. Image reproduced from: [19] In mathematical terms, convolution (\u2217) is an operation between two functions to create a third one, which represents to what extent a function can be modified by another (Equation 2.5). In the convolution stage, a filter matrix (of smaller dimensions than the input image) is applied to the input image in order to produce a feature map. The filter is made move above the image to cover all the pixels (Figure 2.8). Computing a matrix multiplication between the input and the filter, and then adding together the results, the single pixels of the feature map can be produced. A stride value is chosen to determine how much the filter moves during each step. Figure 2.8: Image Convolution. Image reproduced from: [19] Padding is then performed on the resulting feature map (adding pixels of values equal to zero) in order to avoid having a feature map of smaller size than the input image. Finally, after a convolutional layer a max pooling layer is usually applied in order to reduce the size of the input image, so to avoid overfitting (learning just the main features from the input, not also the added noise) and to speed up the model training. Figure 2.9: Max Pooling. Image reproduced from: [20] Performance Metrics There are many types of metrics used nowadays in Artificial Intelligence in order to evaluate a model's overall performance. The three main metrics used in classification problems are: Classification Accuracy Confusion Matrix AUC - ROC Curve Classification Accuracy is the most used metric. It can be calculated by dividing the number of correct predictions by the total number of predictions made. This metric gives a good understanding of the model performance particularly if the data-set is perfectly balanced (all the classes have the same number of samples). A Confusion Matrix is a representation of all the predictions made by the model, compared to their real values. In binary classification (for example considering zero as patient being health and one as being ill), the Confusion Matrix is composed of just four cases: True Negatives (TN) = both the model and the real output are equal to zero. False Positives (FP) = the model predicted one, while the real output was equal to zero. False Negatives (FN) = the model predicted zero, while the real output was equal to one. Trues Positives (TP) = both the model and the real output are equal to one. Table 2.1: Confusion Matrix Using the Confusion Matrix, the model Accuracy can be also calculated as: From the Confusion Matrix it is then possible to calculate a model Sensitivity and Specificity. In a medical context, sensitivity quantifies the model's ability to determine patients effected by a certain medical condition. Specificity instead measures the model ability to correctly identify patients not effected by this condition. The AUC (Area Under The Curve) - ROC (Receiver Operating Characteristics) curve evaluates a model's ability to correctly discriminate between the different classes using different thresholds. Plotting the False Positive Rate (Specificity) against the True Positive Rate (1 - Sensitivity), the ROC Curve can be calculated (Figure 2.10). Figure 2.10: AUC - ROC Curve Examples An AUC of 0.5 would represent the model is randomly guessing between the different classes (as shown in Figure 2.10 (a) by the black dotted line) and an AUC of one would instead show the model is making a perfect classification. If the AUC score would be less than 0.5, that would mean the model is doing exactly the opposite of what it has been designed to do. The ROC curve can be additionally calculated (in a binary classification case) from the data classes probability distributions, calculating the Sensitivity and Specificity (Figure 2.11). Choosing an arbitrary threshold value, the True Negatives will be equal to the Class 1 area on the left side of the threshold. The True Positives will instead be equal to the Class 2 area on the right side of the threshold. The addition of True Negatives and False Positives is represented by the the total area covered by Class 1 while the addition of True Positives and False Negatives is represented be the area covered by Class 2. Figure 2.11: Classes Probability Distributions The AUC would yield better results (closer to 1) if the intersection between the two probability distributions is as low as possible. In fact the model False Positives can be represented as the Class 1 area on the right of the threshold and the False Negatives as the Class 2 area on the left of the threshold. If there are no points in common between the distributions (the two output classes are perfectly distinguishable one from the other) then the AUC would be equal to 1 (Figure 2.12 (a)). As shown in Figure 2.12 (b) if the two distributions are instead perfectly overlapping (having all their points in common), the AUC would be equal to 0.5 (the model is randomly guessing between the two classes). Figure 2.12: Probability distributions and AUC relationship Other types of classification metrics commonly used are: Logarithmic Loss and Classification Report. Ethics AI is now used in many fields. Applications in sectors such as medicine and self-driving vehicles, are raising concerns for private individuals as well as public authorities. This is because, this technology can be used not just as a tool to help humans perform tasks, but also take the place of humans themselves in carrying them out. This can be applied not just for repetitive tasks (such as industry automation), but to more decision making intensive tasks (such as medical prescriptions). There is now therefore a sense of urgency for Explainable AI. Creating models able to explain their conclusions will reassure the public about their trustworthiness and enable developers to understand if they are affected by bias. Companies such as Google, have decided to create a list of ethical principles to be followed when creating AI models [23]. AI should: Be socially beneficial. Avoid creating or reinforcing unfair bias. Be built and tested for safety. Be accountable to people. Incorporate privacy design principles. Uphold high standards of scientific excellence. Be made available for uses that accord with these principles.","title":"Background Theory"},{"location":"chapter2/#background-theory","text":"","title":"Background Theory"},{"location":"chapter2/#autism-spectrum-disorders-asd","text":"Autism Spectrum Disorder is used as an umbrella term to enclose different types of disabilities. Suffering from ASD can lead to many types of difficulties, which can vary vastly between patients, such as communication, social, language and motor disorders. Different types of Autism Spectrum Disorders include: Asperger Syndrome = children affected can have an high intelligence quotient (IQ) , but suffer of social interaction problems. Autistic Disorder = children affected demonstrate unusual behaviours, language delays and communication difficulties. Pervasive developmental disorder, not otherwise specified (PDD-NOS) = children who display some ASD symptoms but do not meet all the Asperger Syndrome or Autistic Disorders criteria. Genes can play an important role in determining if a child is affected or not by autism, but there is still a lot that can be done to prevent it. Mothers who suffer a low iron intake, have an unhealthy lifestyle or are exposed to a high level of air pollution are in fact more likely to give birth to autistic children. Early signs to understand if a child is affected or not by autism can be lack of: happy expressions, response to stimulus, gesturing for communicating and attempts to speak [3] (Figure 2.1). Figure 2.1: Autism Symptoms. Image reproduced from: [4] Many behavioural therapies have been designed during the last few decades in order to aid children affected by autism such as: Floortime, Pivotal Response Treatment (PRT), Verbal Behavior Therapy, ABA and ESDM. During Floortime, parents and therapists take part in games the child likes to play, encouraging the child to take initiatives on how to develop the game and share this activity with other people. In PRT the therapist aims instead to stimulate specific vital developmental areas such as self-management and response to cues. Verbal Behavior Therapy was designed to help a child improve its language and communicative abilities. This is done by making children aware of the importance and benefits of using words as a way to express their needs and desires. Nowadays, there does not exist a recognised medical test for autism diagnostic. Cases are examined individually by doctors for classification. Online screening tools such as Q-Chat are currently available to help parents understand if their child is affected or not by autism [5]. Uses of Machine Learning (to analyse patients EEG readings) and Computer Vision (to detect, from video recording, behavioural and communication impairments) might provide a useful solution to this problem.","title":"Autism Spectrum Disorders (ASD)"},{"location":"chapter2/#applied-behavior-analysis-aba","text":"Applied Behavior Analysis is a therapy approach based on finding out how learning takes place and how behaviour can be affected by the surrounding environment. It's main objective is to encourage behaviours that are positive for the patient and discourage behaviours that can negatively affect learning or have detrimental effects on the patient. One of the main parts ABA is interested in, is to understand what happens before and after the occurrence of a behaviour. ABA therapy can lead to an increase in social, communication skills and attention. This type of intervention approach can be used not only to help children (and potentially also adults) with autism but also other types of physical behaviour disorders such as eating disorders. ABA efficacy has been successfully proven thanks to many years of studies accomplished by acclaimed institutions and researchers [6].","title":"Applied Behavior Analysis (ABA)"},{"location":"chapter2/#early-start-denver-model-esdm","text":"The Early Start Denver Model is a teaching play based behaviour therapy used to help children with disabilities. This approach is based on ABA, and is aimed at children between 1 to 4 years old. The main objective of ESDM is to help children improve their communication, language and social skills. This is achieved through games and joint activities. ESDM is typically used in both a domestic and clinical setting (additionally also in schools). In this way, both therapist and parents get personally involved in the therapy. Conducting part of the therapy at home (between children and parents) can also lead to substantial economical savings because it consistently reduces the number of weekly hospital hours of treatment needed (traditionally about 25 hours per week). Additionally, thanks to ESDM, children can also achieve better results because they might feel more comfortable to play at home with their parents rather than with a doctor in a hospital [7]. ESDM approach relies on one to one sessions between therapist and child and personalised intervention tailored according to the needs and improvements of the child. Studies have shown that ESDM can lead to more substantial improvements in children's IQ and adaptive behaviour compared to typical community-intervention approaches [8].","title":"Early Start Denver Model (ESDM)"},{"location":"chapter2/#electroencephalogram-eeg","text":"EEG uses electrodes (small metal disks) placed on the scalp of the patient to track brainwave patterns and send the results to a computer for later analysis. Brain waves are observed measuring the electrical activity in the brain (electrical pulses of neurons communicating each other). EEG is generally used to identify occurrences such as changes of behaviour and seizures. Brainwaves can be divided in different bands according to their travelling speed (Hertz) [9]: Infra-Low: less than 0.5 Hz = basic cortical rythm Delta Waves: 0.5 to 3 Hz = dreamless sleep Theta Waves: 3 to 8 Hz = sleep and meditation Alpha Waves: 8 to 12 Hz = brain resting, flow of thoughts Beta Waves: 12 to 38 Hz = standard consciousness waking state Gamma Waves: 38 to 42 Hz = love, altruism etc... Another form of EEG analysis is quantitative EEG (qEEG). QEEG aims to analyse brainwaves using statistic and mathematics. Thanks to a process of quantitative comparison and accurate measurements, qEEG is able to recreate a map of the brain. A qEEG application example can be a research carried out by Lineu C. Fonseca about \"Quantitative EEG in children with learning disabilities\" [10], which has shown that children who suffer with reading and/or writing disabilities had higher delta activity in frontal-temporal regions than average.","title":"Electroencephalogram (EEG)"},{"location":"chapter2/#serious-games-and-gamification","text":"One of the aims of this project was to design a suite of games to help children with disabilities. Games created for a purpose other than entertainment are notoriously known as Serious Games. While playing a Serious Game, players will be likely asked to solve problems. Instead, traditional games mainly focus on action. In order to make learning effective, the main component required is engagement with the subject. Different mediums for learning are nowadays used such as: reading books, articles, manuals and educational videos. Each of these mediums have their advantages and disadvantages; the strength of delivering educational content through a game-based environment is the increase of student's engagement. What contributes to that are factors such as: the presence of a story-line, challenge and competition against themselves and others to improve their game scores, taking risky decision and paying the consequences for their actions, and learning to follow and understand rules. These concepts have applications in both games and in the real world, users makes mistakes and learn from them to make improvements. Serious Games can have a wide ambit of application as: health-care, defence, scientific and educational purpose's. Multinational companies have made use of Serious Games during the last few years. Examples include: \"Microsoft Flight Simulator\", \"IBM city one\" and \"Amnesty the Game\" [11]. Another application of game mechanics in non-gaming contexts is Gamification. In ordinary use, the terms Serious Games and Gamification get confused with each-other. Gamification aims to examine what engages most its potential players and then builds a game based on the gathered data. Designing games suited for each individual according to their needs and abilities can lead to positive behavioural improvements and increase player motivation. In contrast, Serious Games take business concepts and attempts to apply them using gaming techniques. Serious Games are not generally used as an exclusive learning technique but to reinforce understanding [12]. Figure 2.2: Serious gaming applications growth in academia and industry - Image reproduced from [13] - \"An overview of Serious Games\" - Fedwa Laamarti et al.","title":"Serious Games and Gamification"},{"location":"chapter2/#artificial-intelligence-ai","text":"The birth of AI goes back to the end of the Second World War, starting with Alan Turing. At that point in time, German forces were using the Enigma Machine to encrypt their messages. Turing, using the Bombe machine, successfully managed to crack the 'Enigma' code. These two machines (Enigma and Bombe) constituted the foundations of Machine Learning. In fact, according to the Turing Test, a machine could be considered as intelligent if it is able to converse with humans without letting them find out it's true nature (therefore winning the \"imitation game\") [14]. Today, Artificial Intelligence has undergone impressive advancements. AI can be subdivided into three different levels according to the ability of machines to perform intellectual tasks logically and independently: Narrow AI = machines are more efficient than humans in performing very specific tasks (but not trying to perform other types of tasks). General AI = machines are as intelligent as human beings. Strong AI = machines perform better than humans in different ambit (in tasks that we might or not be able to perform at all). Right now we are only at a Narrow AI level. Artificial Intelligence can be broken down in many different categories such as: Machine Learning, Natural Language Processing (NLP), Vision, Robotics and Autonomous Vehicles [15] (Figure 2.3). Figure 2.3: Different AI fields. Image reproduced from: [15] One of the most frequently used AI tools is Machine Learning. There are three main types of machine learning algorithms used: Supervised Learning = using a labelled training set to train a model, to then make predictions on unlabelled data. Unsupervised Learning = giving a model an unlabelled data-set, the model has then to try to find patterns in the data to make predictions. Reinforcement Learning = training a model trough a reward mechanism to encourage positive behaviours in case of good performance (particularly used in robotics applications). The most important sub-field of Machine Learning is Deep Learning (DL). DL is mainly inspired by the structure and function of the human brain and therefore makes use of artificial neural networks. One of the first attempts in this field was the perceptron (which had very limited performance). Successive development of deep neural networks, adding multiple hidden units, using non linear activation functions and back-propagation algorithms lead to far greater results. Figure 2.4: AI, ML and DL relation. Image reproduced from: [16]","title":"Artificial Intelligence (AI)"},{"location":"chapter2/#long-short-term-memory-lstm","text":"LSTM is a type of Recurrent Neural Network (RNN). RNN were initially developed to make up for the inability of traditional neural networks to work with time series. This is because traditional neural networks, in contrast with RNN, lack of a memory mechanism to remember its past input values (all the inputs are fed in at once and information moves only in one direction; forward). RNN are able to overcome these shortfalls keeping information in a loop, therefore enabling the network to make decisions using not only the current input but also the previous ones (input are added sequentially, one input is added after the previous one has been computed). Therefore, to some extent, a RNN can be considered as a collection of artificial neural networks passing each output from the previous one. Figure 2.5: Artificial Neural Network equivalent of RNN Cell. Image reproduced from [17] RNN are although still far from being perfect. They are in fact just able to retain short term information and might suffer from vanishing/exploding gradients when performing the backwards pass. In order to solve these kinds of problem, the LSTM architecture was created. LSTMs are used to tackle time series forecasting, speech recognition and music generation tasks. The difference between an RNN and an LSTM lies in the repeating module (A in Figure 2.5). LSTM makes use of a far more structured architecture (Figure 2.6). Each module has three inputs: the current time step, an input from the previous module and the memory from the previous module. This way, each module can alter or preserve the network memory. Figure 2.6: LSTM Module. Image reproduced from: [18] Each LSTM module can be considered to be formed by three gates: input (I t ) , forget (f t ) and output gate (o t ). The input gate decides if a piece of information is important enough or not to be remembered (Equation 2.1). The forget gate (Equation 2.2) decides if a piece of information stored is still relevant or not (and therefore has to be deleted). The output gate determines if a particular information has to have or not a weight in the current time step (Equation 2.3). The Sigmoid function (\u03c3, Equation 2.4) is used to squish the output between any value from zero (making the gate block everything) to one (making the gate pass through everything). A neuron gate weight (w x ) represents the strength of the connection, while the bias (b x ) is used shift the activation function to fit best the data.","title":"Long Short-Term Memory (LSTM)"},{"location":"chapter2/#convolutional-neural-network-cnn","text":"CNN are a class of neural networks typically used for image recognition/classification, but can be also used for time series analysis. This can be done by converting the time series in a grayscale image like format. The network takes as input a three-dimensional array (Height \u00d7 Width \u00d7 Depth) representing an image. A CNN is composed in two main sections: a feature extraction part and a classification part. During the feature extraction part, convolutions and padding layers are applied to extract the main characteristics embedded in the input vector. In the classification part, fully connected layers (converts our three-dimensional input into a one-dimensional array) are then used applying a Softmax function to classify the detected features. Figure 2.7: CNN structure. Image reproduced from: [19] In mathematical terms, convolution (\u2217) is an operation between two functions to create a third one, which represents to what extent a function can be modified by another (Equation 2.5). In the convolution stage, a filter matrix (of smaller dimensions than the input image) is applied to the input image in order to produce a feature map. The filter is made move above the image to cover all the pixels (Figure 2.8). Computing a matrix multiplication between the input and the filter, and then adding together the results, the single pixels of the feature map can be produced. A stride value is chosen to determine how much the filter moves during each step. Figure 2.8: Image Convolution. Image reproduced from: [19] Padding is then performed on the resulting feature map (adding pixels of values equal to zero) in order to avoid having a feature map of smaller size than the input image. Finally, after a convolutional layer a max pooling layer is usually applied in order to reduce the size of the input image, so to avoid overfitting (learning just the main features from the input, not also the added noise) and to speed up the model training. Figure 2.9: Max Pooling. Image reproduced from: [20]","title":"Convolutional Neural Network (CNN)"},{"location":"chapter2/#performance-metrics","text":"There are many types of metrics used nowadays in Artificial Intelligence in order to evaluate a model's overall performance. The three main metrics used in classification problems are: Classification Accuracy Confusion Matrix AUC - ROC Curve Classification Accuracy is the most used metric. It can be calculated by dividing the number of correct predictions by the total number of predictions made. This metric gives a good understanding of the model performance particularly if the data-set is perfectly balanced (all the classes have the same number of samples). A Confusion Matrix is a representation of all the predictions made by the model, compared to their real values. In binary classification (for example considering zero as patient being health and one as being ill), the Confusion Matrix is composed of just four cases: True Negatives (TN) = both the model and the real output are equal to zero. False Positives (FP) = the model predicted one, while the real output was equal to zero. False Negatives (FN) = the model predicted zero, while the real output was equal to one. Trues Positives (TP) = both the model and the real output are equal to one. Table 2.1: Confusion Matrix Using the Confusion Matrix, the model Accuracy can be also calculated as: From the Confusion Matrix it is then possible to calculate a model Sensitivity and Specificity. In a medical context, sensitivity quantifies the model's ability to determine patients effected by a certain medical condition. Specificity instead measures the model ability to correctly identify patients not effected by this condition. The AUC (Area Under The Curve) - ROC (Receiver Operating Characteristics) curve evaluates a model's ability to correctly discriminate between the different classes using different thresholds. Plotting the False Positive Rate (Specificity) against the True Positive Rate (1 - Sensitivity), the ROC Curve can be calculated (Figure 2.10). Figure 2.10: AUC - ROC Curve Examples An AUC of 0.5 would represent the model is randomly guessing between the different classes (as shown in Figure 2.10 (a) by the black dotted line) and an AUC of one would instead show the model is making a perfect classification. If the AUC score would be less than 0.5, that would mean the model is doing exactly the opposite of what it has been designed to do. The ROC curve can be additionally calculated (in a binary classification case) from the data classes probability distributions, calculating the Sensitivity and Specificity (Figure 2.11). Choosing an arbitrary threshold value, the True Negatives will be equal to the Class 1 area on the left side of the threshold. The True Positives will instead be equal to the Class 2 area on the right side of the threshold. The addition of True Negatives and False Positives is represented by the the total area covered by Class 1 while the addition of True Positives and False Negatives is represented be the area covered by Class 2. Figure 2.11: Classes Probability Distributions The AUC would yield better results (closer to 1) if the intersection between the two probability distributions is as low as possible. In fact the model False Positives can be represented as the Class 1 area on the right of the threshold and the False Negatives as the Class 2 area on the left of the threshold. If there are no points in common between the distributions (the two output classes are perfectly distinguishable one from the other) then the AUC would be equal to 1 (Figure 2.12 (a)). As shown in Figure 2.12 (b) if the two distributions are instead perfectly overlapping (having all their points in common), the AUC would be equal to 0.5 (the model is randomly guessing between the two classes). Figure 2.12: Probability distributions and AUC relationship Other types of classification metrics commonly used are: Logarithmic Loss and Classification Report.","title":"Performance Metrics"},{"location":"chapter2/#ethics","text":"AI is now used in many fields. Applications in sectors such as medicine and self-driving vehicles, are raising concerns for private individuals as well as public authorities. This is because, this technology can be used not just as a tool to help humans perform tasks, but also take the place of humans themselves in carrying them out. This can be applied not just for repetitive tasks (such as industry automation), but to more decision making intensive tasks (such as medical prescriptions). There is now therefore a sense of urgency for Explainable AI. Creating models able to explain their conclusions will reassure the public about their trustworthiness and enable developers to understand if they are affected by bias. Companies such as Google, have decided to create a list of ethical principles to be followed when creating AI models [23]. AI should: Be socially beneficial. Avoid creating or reinforcing unfair bias. Be built and tested for safety. Be accountable to people. Incorporate privacy design principles. Uphold high standards of scientific excellence. Be made available for uses that accord with these principles.","title":"Ethics"},{"location":"chapter3/","text":"Gaming Platform Hardware implementation In regard to a hardware solution, to help children's with disabilities, I decided to make a projector using a DLPDLCR2000EVM Evaluation Board and a Raspberry-Pi 3B (running the Raspbian operating system). Succesively, I managed to then make the area projected touchscreen using a Wiimote (Wii Remote controller [24]) and an Infrared (IR) pen I built myself. The Wiimote is connected via Bluetooth (to the Raspberry-Pi) and is used as an infrared receiver to detect the activities of the infrared pen on the projected surface. Finally, I programmed a range of games that can be loaded on the Raspberry-Pi for children to play. In order to wire and configure the connections between the Evaluation Board and the Raspberry-Pi, I followed an online guide by Frederick Vandenbosch [25] (Appendix A). I then set up the Raspberry Pi Bluetooth (to enable the Wiimote connection) by connecting the Bluetooth adapter and then installing the Bluez and Wiimote integration packages through Linux terminal. Lastly, I installed a free python-whiteboard software by Martin Rudel [26] to be used in order to calibrate the surface of the projected area. It is also possible to implement this solution on Windows using a standard projector and the free whiteboard calibration software Smoothboard Air with Duo [27]. Figure 3.1: Infrared Pen, DLPDLCR2000EVM Evaluation Board and Raspberry Pi Figure 3.2 shows the two different playing modes which can be used. In the first one, the projected area can be used as a whiteboard, while in the second one the user can choose to play far from the surface using the infrared pen as a pointer. Choosing the playing mode to use is important to pick the right calibration position of the Wiimote. Figure 3.2: Touchscreen projector playing modes This hardware interface was mainly designed to help children affected by Developmental Coordination Disorders. This type of motor disorder causes coordination problems (eg. wrong movement timing/balance) due to incorrect communication between the brain and the body. First Game Development I decided firstly to make a game designed for children with cognitive disabilities to test both their rate of reception and their ability to work with others. In order to create this game I made use of references such as \\\"Python game development\\\" [28] and \\\"Beginning Game Development with Python and pygame\\\" [29]. I decided to program this game using Python as programming language and pygame as my main working library. Figure 3.3(a) shows the menu interface I designed for the game. I decided to name the game \\\"Avalanche Escape\\\", since it consists of moving characters around the graphical user interface (GUI) to avoid falling snowflakes. The GUI can be considered as a Cartesian coordinate system. During the game, the snowflakes fall from the top of the window, always changing their Y axis starting position. The snowflakes velocity slowly increases every-time the character successfully manages to avoid them (Figure 3.3(b)). This way the game will seem relatively easy to play at the beginning but as the user practises and gets better at playing it, the game difficulty will also increase. It is possible to read the game instructions using the game instruction button present on the menu. Figure 3.3: Avalanche escape In the menu the user can choose to play in either single or multi player mode. In single player mode, the male character is moved by either using the mouse or keyboard arrows. In multi player mode, the female character is instead moved by using the mouse and the male one by using the keyboard. If one of the characters is hit by a snowflake, a menu will prompt warning of the collision and ask if the user wants to play again or go back to the main menu. If the user manages to avoid the predetermined number of snowflakes asked by the task, a menu will appear on the screen congratulating the user for winning. Each of these actions are accompanied by sounds which are correspondent to whether the user wins or loses the game in order to increase user engagement. Throughout the whole playing time, there is music playing in the background. The user can pause the game at any time by pressing the keyboard escape button and switch between full-screen display mode (pressing F) and standard screen mode (pressing N). A computer play mode is also present which can be used to demonstrate to the user how to play. In order to implement the computer play mode I used a simple algorithm. Using Machine Learning instead of traditional coding could potentially lead to better gaming performance and score. Finally, I made an executable version of this game which can be easily installed on any type of Windows platform using the cx_Freeze python library. Imitation Games Imitation is the ability to mimic other people behaviours. Children usually start imitating others from around the age of two and to mimic different types of behaviour at different age stages. The ability to imitate is not something inherently transmitted but is instead the result of our human social life organisation. Studies demonstrate that children with autism usually suffer delays in their Imitation and Joint Attention abilities. Imitation skills can play a fundamental role in a child's behavioural development. During this project, I decided to design different games to try to stimulate and enhance these kind of abilities. Second Game Development This game is divided into two different modes (Figure 3.4). When selecting the first mode, the child is asked to recognise the animals in the pictures (Figure 3.5 (a)). Clicking on each of the different buttons an audio will play the sound of the corresponding animal and a message box will appear. The child will then be asked to type in a text-box the appropriate animal name. If the child types the right animal name (it doesn't matter if the name is written in all lowercase, all uppercase or a in mixture of them) a congratulations message box would appear. If instead the the child types the wrong answer, it would be encouraged to try again. Figure 3.4: Game 2 Menu When selecting the second mode (Figure 3.5 (b)), the child is instead asked to recognise and then write the animals names, this time by just hearing without corresponding images (therefore increasing the game difficulty). When playing either of the game modes the child will be encouraged by the parent/therapist to try to talk about the identified animals and to imitate their sound. Figure 3.5: Game Mode Options This game therefore aims to improve a child's ability to recognise and imitate animal sounds in the first mode and in the second one to guess (encouraging them to first take a risk and then learn from his/her mistakes) and imitate animals sounds. Different levels of difficulty can eventually be designed as further development of this game. The modes designed so far are aimed to not be too difficult, in order to try to not discourage a child from playing (if the game would have been too difficult from the start, a child might lose interest). This game was designed using Python and libraries such as Tkinter, pygame and PIL. Third Game Development The aim of this game is to encourage children to imitate the therapist/parent's actions and to build objects/shapes (Figure 3.6). The game GUI consists of two canvases which can interchangeably be used by either the child or therapist/parent. Selecting the correspondent buttons, the users would be able to make appear on the screen up to six blocks which they can drop and drag around the screen to create objects or shapes. If an user tries to place a block either outside the game window or in the canvas of the other player, he/she will be stopped by a warning prompt asking him to try again. While playing this game, the parent/therapist should build one or more object(s) using the given blocks on one canvas and the child should then try to imitate the same object(s) on his canvas either while the parent/therapist is making it or after. Figure 3.6: Game 3 -4ex Once the child has come up with his solution to this problem, it is then possible to store the result by clicking the \\\"Save\\\" button on the GUI. A screen-shot will then be generated and stored as a record of the child's performance. Keeping a record of all the different attempts the child makes while trying to build different types of objects, is then possible to see whether the child has improved over time and understand if the child has a problem imitating certain specific shapes or patterns. I also decided to implement an EXE version of this game to make implementation easier. This game was designed using Python and libraries such as pyoutogui, pygame and time. Fourth Game Development This game was designed to help children develop their recognition and matching skills solving puzzles (Figure 3.7). Figure 3.7: Game 4 Menu I decided to design this game using Unity for Graphic Development and C# as programming language for coding the graphics behaviour. I decided to opt for this solution because it enabled me to make available this game not just for desktop platforms (like in all the other games made so far), but also on Android devices (eg. mobile phones and tablets). This was made possible by creating both an EXE and APK format of the game. Both versions have been successfully tested on both platforms. Figure 3.8: Game Mode Clicking (or tapping using a touchscreen Android device) on each of the images that composes the puzzle makes them rotate by 90clockwise (Figure 3.8 (a)). Once the child has selected the right orientation for each of the puzzle images, a congratulations text message appears on the screen and the user is redirected to the main menu where they can decide if to play again or end the game (Figure 3.8 (b)). Joint Attention Games Joint attention (JA) is the ability of two individuals to share their interest on the same event, person or object. This implies an underlying understanding between the two people in question. This can be achieved through any form of verbal or non-verbal indication such as pointing or eye-gazing. In JA, a child follows the lead of the parent/therapist and tries to copy or mimic them. JA allows children to share enjoyment and learn from others. Children usually start showing the first forms of JA around the age of 9 months old. Children affected by autism symptoms might instead show JA abilities later on or, in some cases, might never fully develop them. To external viewers, this might look like a lack of engagement in collaborating and interacting with other people. There are two main forms of JA: Child initiating JA Child responding to call of JA from another person Fifth Game Development I decided to structure this game with four different levels of difficulty (Figure 3.9). This game consists of identifying the right objects between the given options (there can be either one or more right answers) and recording the child's justification for their decision. It's main application is to improve child's receptive communication. Figure 3.9: Game 5 Menu If the child selects the right answer, a congratulations prompt will appear. The prompt will inform the child he/she made the right choice and will then inform them that closing it will start the recording. The recording will continue until the child stops speaking. For each time the child selects the right answer, the recording will be stored and could then be used for later analysis by the therapist. The recording will play a vital role in evaluating the understanding of the child and track any signs of improvement. If the child selects instead a wrong answer, a prompt will encourage him/her to try again (Figure 3.10 (a)). Figure 3.10: Game Mode Options For the fourth difficulty level of this game, I decided to make the buttons interactive using instead of plain images, animated GIFs. Doing so I aimed to make the game more stimulating and interactive for the child (Figure 3.10 (b)). This game was designed using Python and libraries such as Tkinter, PIL, itertools, pyaudio and wave. Sixth Game Development This game aims to help children improve their receptive communication, in particular their understanding of quantities and numbers. I decided to design this game using Microsoft Visual Studio and C#, which facilitated the creation of the final EXE file. The menu interface (Figure 3.11 (a)) offers apart from the main buttons also a top drop down menu which can give additional help for the child to learn about the game. At the bottom of the GUI is also present a status bar, offering additional information to the user about the meaning of the different menu options (once clicked on them) and to keep the user updated about the status of the game. As shown in Figure 3.11 (b), during each game level are tested the child's abilities to identify the button showing the asked right number of objects displayed in the pictures. Figure 3.11: Game Six Options Seventh Game Development The seventh game I decided to design was a drawing canvas for two users (Figure 3.12). I decided to program this game using Python and the Tkinter graphics library. The two canvases consist of two independent drawing surfaces. This way, it is either possible for the user to make free drawings when playing on their own or to let the therapist/parent draw something on one of the two canvas and then let the child imitate their drawing on the other canvas. It is additionally possible, when playing with the therapist/parent, to let the fist write or pronounce a word and then let the child draw the corresponding image. This type of game aims to help children learn through imitation and respond to external stimulus. Figure 3.12: Drawing canvas for two users As can be seen from Figure 3.12, the GUI gives the users the choice to: select their preferred font size (bottom centre slider), favourite colour (left-bottom button), switch between pen and eraser mode (top centre button, right bottom button) and to clear the canvases (top left button for left canvas and top right button for right canvas). EEG Cap Having designed a suite of games and tested they functioned correctly using them with the hardware implementation, I started working to connect, synchronise, and store the data from the EEG Cap. In my case, I have been working with the Enobio 20 (produced by Neuroelectronics [30]), a wearable, wireless sensor system used for recording electroencephalogram (EEG) signals. In order to connect and control the Enobio 20 using my laptop, I first had to install the Neuroelectrics Instrument Controller (NIC) free software offered by Neuroelectronics and then connect my laptop to the Enobio 20 via bluetooth using the NIC. Once I made sure everything was set up between my laptop and the Enobio 20, I started working on trying to control the Enobio 20 calling a MATLAB script instead of having to use the NIC. To do so, I made use of the MatNIC MATLAB toolbox produced by Neuroelectronics [31]. Making a MATLAB script calling some of the many available MatNIC functions, I managed to automate the process of: opening the NIC software, connecting the Enobio 20 via bluetooth, starting the EEG readings for a chosen predetermined amount of time and output the final reading results in a comma-separated values (CSV) file format. When calling the MATLAB script, it is still necessary to press \\\"Enter\\\" to allow the NIC software to be controlled remotely. This has been included by Neuroelectrics for security reasons, in order to avoid giving unauthorised access to the EEG Readings. Using the MatNICEEGRecord function (keeping TCP connection enabled), the CSV file is composed by as many columns as the channels of the device (in this case 20) and two more columns, one for the markers (if no markers have been sent, this column will be composed just by zeroes) and one for the timestamp. If instead the MatNICEEGRecordLSL function is used (through Lab Streaming Layer communication), the CSV file is composed by as many columns as the channels of the device (in this case 20). The units of the measurements for each channel are in nano-Volts (nV). Finally, I managed to synchronise the seventh game I developed with the EEG readings calling the MATLAB script I made, using Python. I therefore modified the code for the drawing canvas for two users by adding the matlab.engine library to call the MATLAB script file and using the threading library to make the canvas game and the EEG reading run simultaneously (without dividing the code into threads they would have run sequentially, compromising synchronisation). The main functions described above are available in Appendix B, Listings 1 and 2.","title":"Gaming Platform"},{"location":"chapter3/#gaming-platform","text":"","title":"Gaming Platform"},{"location":"chapter3/#hardware-implementation","text":"In regard to a hardware solution, to help children's with disabilities, I decided to make a projector using a DLPDLCR2000EVM Evaluation Board and a Raspberry-Pi 3B (running the Raspbian operating system). Succesively, I managed to then make the area projected touchscreen using a Wiimote (Wii Remote controller [24]) and an Infrared (IR) pen I built myself. The Wiimote is connected via Bluetooth (to the Raspberry-Pi) and is used as an infrared receiver to detect the activities of the infrared pen on the projected surface. Finally, I programmed a range of games that can be loaded on the Raspberry-Pi for children to play. In order to wire and configure the connections between the Evaluation Board and the Raspberry-Pi, I followed an online guide by Frederick Vandenbosch [25] (Appendix A). I then set up the Raspberry Pi Bluetooth (to enable the Wiimote connection) by connecting the Bluetooth adapter and then installing the Bluez and Wiimote integration packages through Linux terminal. Lastly, I installed a free python-whiteboard software by Martin Rudel [26] to be used in order to calibrate the surface of the projected area. It is also possible to implement this solution on Windows using a standard projector and the free whiteboard calibration software Smoothboard Air with Duo [27]. Figure 3.1: Infrared Pen, DLPDLCR2000EVM Evaluation Board and Raspberry Pi Figure 3.2 shows the two different playing modes which can be used. In the first one, the projected area can be used as a whiteboard, while in the second one the user can choose to play far from the surface using the infrared pen as a pointer. Choosing the playing mode to use is important to pick the right calibration position of the Wiimote. Figure 3.2: Touchscreen projector playing modes This hardware interface was mainly designed to help children affected by Developmental Coordination Disorders. This type of motor disorder causes coordination problems (eg. wrong movement timing/balance) due to incorrect communication between the brain and the body.","title":"Hardware implementation"},{"location":"chapter3/#first-game-development","text":"I decided firstly to make a game designed for children with cognitive disabilities to test both their rate of reception and their ability to work with others. In order to create this game I made use of references such as \\\"Python game development\\\" [28] and \\\"Beginning Game Development with Python and pygame\\\" [29]. I decided to program this game using Python as programming language and pygame as my main working library. Figure 3.3(a) shows the menu interface I designed for the game. I decided to name the game \\\"Avalanche Escape\\\", since it consists of moving characters around the graphical user interface (GUI) to avoid falling snowflakes. The GUI can be considered as a Cartesian coordinate system. During the game, the snowflakes fall from the top of the window, always changing their Y axis starting position. The snowflakes velocity slowly increases every-time the character successfully manages to avoid them (Figure 3.3(b)). This way the game will seem relatively easy to play at the beginning but as the user practises and gets better at playing it, the game difficulty will also increase. It is possible to read the game instructions using the game instruction button present on the menu. Figure 3.3: Avalanche escape In the menu the user can choose to play in either single or multi player mode. In single player mode, the male character is moved by either using the mouse or keyboard arrows. In multi player mode, the female character is instead moved by using the mouse and the male one by using the keyboard. If one of the characters is hit by a snowflake, a menu will prompt warning of the collision and ask if the user wants to play again or go back to the main menu. If the user manages to avoid the predetermined number of snowflakes asked by the task, a menu will appear on the screen congratulating the user for winning. Each of these actions are accompanied by sounds which are correspondent to whether the user wins or loses the game in order to increase user engagement. Throughout the whole playing time, there is music playing in the background. The user can pause the game at any time by pressing the keyboard escape button and switch between full-screen display mode (pressing F) and standard screen mode (pressing N). A computer play mode is also present which can be used to demonstrate to the user how to play. In order to implement the computer play mode I used a simple algorithm. Using Machine Learning instead of traditional coding could potentially lead to better gaming performance and score. Finally, I made an executable version of this game which can be easily installed on any type of Windows platform using the cx_Freeze python library.","title":"First Game Development"},{"location":"chapter3/#imitation-games","text":"Imitation is the ability to mimic other people behaviours. Children usually start imitating others from around the age of two and to mimic different types of behaviour at different age stages. The ability to imitate is not something inherently transmitted but is instead the result of our human social life organisation. Studies demonstrate that children with autism usually suffer delays in their Imitation and Joint Attention abilities. Imitation skills can play a fundamental role in a child's behavioural development. During this project, I decided to design different games to try to stimulate and enhance these kind of abilities.","title":"Imitation Games"},{"location":"chapter3/#second-game-development","text":"This game is divided into two different modes (Figure 3.4). When selecting the first mode, the child is asked to recognise the animals in the pictures (Figure 3.5 (a)). Clicking on each of the different buttons an audio will play the sound of the corresponding animal and a message box will appear. The child will then be asked to type in a text-box the appropriate animal name. If the child types the right animal name (it doesn't matter if the name is written in all lowercase, all uppercase or a in mixture of them) a congratulations message box would appear. If instead the the child types the wrong answer, it would be encouraged to try again. Figure 3.4: Game 2 Menu When selecting the second mode (Figure 3.5 (b)), the child is instead asked to recognise and then write the animals names, this time by just hearing without corresponding images (therefore increasing the game difficulty). When playing either of the game modes the child will be encouraged by the parent/therapist to try to talk about the identified animals and to imitate their sound. Figure 3.5: Game Mode Options This game therefore aims to improve a child's ability to recognise and imitate animal sounds in the first mode and in the second one to guess (encouraging them to first take a risk and then learn from his/her mistakes) and imitate animals sounds. Different levels of difficulty can eventually be designed as further development of this game. The modes designed so far are aimed to not be too difficult, in order to try to not discourage a child from playing (if the game would have been too difficult from the start, a child might lose interest). This game was designed using Python and libraries such as Tkinter, pygame and PIL.","title":"Second Game Development"},{"location":"chapter3/#third-game-development","text":"The aim of this game is to encourage children to imitate the therapist/parent's actions and to build objects/shapes (Figure 3.6). The game GUI consists of two canvases which can interchangeably be used by either the child or therapist/parent. Selecting the correspondent buttons, the users would be able to make appear on the screen up to six blocks which they can drop and drag around the screen to create objects or shapes. If an user tries to place a block either outside the game window or in the canvas of the other player, he/she will be stopped by a warning prompt asking him to try again. While playing this game, the parent/therapist should build one or more object(s) using the given blocks on one canvas and the child should then try to imitate the same object(s) on his canvas either while the parent/therapist is making it or after. Figure 3.6: Game 3 -4ex Once the child has come up with his solution to this problem, it is then possible to store the result by clicking the \\\"Save\\\" button on the GUI. A screen-shot will then be generated and stored as a record of the child's performance. Keeping a record of all the different attempts the child makes while trying to build different types of objects, is then possible to see whether the child has improved over time and understand if the child has a problem imitating certain specific shapes or patterns. I also decided to implement an EXE version of this game to make implementation easier. This game was designed using Python and libraries such as pyoutogui, pygame and time.","title":"Third Game Development"},{"location":"chapter3/#fourth-game-development","text":"This game was designed to help children develop their recognition and matching skills solving puzzles (Figure 3.7). Figure 3.7: Game 4 Menu I decided to design this game using Unity for Graphic Development and C# as programming language for coding the graphics behaviour. I decided to opt for this solution because it enabled me to make available this game not just for desktop platforms (like in all the other games made so far), but also on Android devices (eg. mobile phones and tablets). This was made possible by creating both an EXE and APK format of the game. Both versions have been successfully tested on both platforms. Figure 3.8: Game Mode Clicking (or tapping using a touchscreen Android device) on each of the images that composes the puzzle makes them rotate by 90clockwise (Figure 3.8 (a)). Once the child has selected the right orientation for each of the puzzle images, a congratulations text message appears on the screen and the user is redirected to the main menu where they can decide if to play again or end the game (Figure 3.8 (b)).","title":"Fourth Game Development"},{"location":"chapter3/#joint-attention-games","text":"Joint attention (JA) is the ability of two individuals to share their interest on the same event, person or object. This implies an underlying understanding between the two people in question. This can be achieved through any form of verbal or non-verbal indication such as pointing or eye-gazing. In JA, a child follows the lead of the parent/therapist and tries to copy or mimic them. JA allows children to share enjoyment and learn from others. Children usually start showing the first forms of JA around the age of 9 months old. Children affected by autism symptoms might instead show JA abilities later on or, in some cases, might never fully develop them. To external viewers, this might look like a lack of engagement in collaborating and interacting with other people. There are two main forms of JA: Child initiating JA Child responding to call of JA from another person","title":"Joint Attention Games"},{"location":"chapter3/#fifth-game-development","text":"I decided to structure this game with four different levels of difficulty (Figure 3.9). This game consists of identifying the right objects between the given options (there can be either one or more right answers) and recording the child's justification for their decision. It's main application is to improve child's receptive communication. Figure 3.9: Game 5 Menu If the child selects the right answer, a congratulations prompt will appear. The prompt will inform the child he/she made the right choice and will then inform them that closing it will start the recording. The recording will continue until the child stops speaking. For each time the child selects the right answer, the recording will be stored and could then be used for later analysis by the therapist. The recording will play a vital role in evaluating the understanding of the child and track any signs of improvement. If the child selects instead a wrong answer, a prompt will encourage him/her to try again (Figure 3.10 (a)). Figure 3.10: Game Mode Options For the fourth difficulty level of this game, I decided to make the buttons interactive using instead of plain images, animated GIFs. Doing so I aimed to make the game more stimulating and interactive for the child (Figure 3.10 (b)). This game was designed using Python and libraries such as Tkinter, PIL, itertools, pyaudio and wave.","title":"Fifth Game Development"},{"location":"chapter3/#sixth-game-development","text":"This game aims to help children improve their receptive communication, in particular their understanding of quantities and numbers. I decided to design this game using Microsoft Visual Studio and C#, which facilitated the creation of the final EXE file. The menu interface (Figure 3.11 (a)) offers apart from the main buttons also a top drop down menu which can give additional help for the child to learn about the game. At the bottom of the GUI is also present a status bar, offering additional information to the user about the meaning of the different menu options (once clicked on them) and to keep the user updated about the status of the game. As shown in Figure 3.11 (b), during each game level are tested the child's abilities to identify the button showing the asked right number of objects displayed in the pictures. Figure 3.11: Game Six Options","title":"Sixth Game Development"},{"location":"chapter3/#seventh-game-development","text":"The seventh game I decided to design was a drawing canvas for two users (Figure 3.12). I decided to program this game using Python and the Tkinter graphics library. The two canvases consist of two independent drawing surfaces. This way, it is either possible for the user to make free drawings when playing on their own or to let the therapist/parent draw something on one of the two canvas and then let the child imitate their drawing on the other canvas. It is additionally possible, when playing with the therapist/parent, to let the fist write or pronounce a word and then let the child draw the corresponding image. This type of game aims to help children learn through imitation and respond to external stimulus. Figure 3.12: Drawing canvas for two users As can be seen from Figure 3.12, the GUI gives the users the choice to: select their preferred font size (bottom centre slider), favourite colour (left-bottom button), switch between pen and eraser mode (top centre button, right bottom button) and to clear the canvases (top left button for left canvas and top right button for right canvas).","title":"Seventh Game Development"},{"location":"chapter3/#eeg-cap","text":"Having designed a suite of games and tested they functioned correctly using them with the hardware implementation, I started working to connect, synchronise, and store the data from the EEG Cap. In my case, I have been working with the Enobio 20 (produced by Neuroelectronics [30]), a wearable, wireless sensor system used for recording electroencephalogram (EEG) signals. In order to connect and control the Enobio 20 using my laptop, I first had to install the Neuroelectrics Instrument Controller (NIC) free software offered by Neuroelectronics and then connect my laptop to the Enobio 20 via bluetooth using the NIC. Once I made sure everything was set up between my laptop and the Enobio 20, I started working on trying to control the Enobio 20 calling a MATLAB script instead of having to use the NIC. To do so, I made use of the MatNIC MATLAB toolbox produced by Neuroelectronics [31]. Making a MATLAB script calling some of the many available MatNIC functions, I managed to automate the process of: opening the NIC software, connecting the Enobio 20 via bluetooth, starting the EEG readings for a chosen predetermined amount of time and output the final reading results in a comma-separated values (CSV) file format. When calling the MATLAB script, it is still necessary to press \\\"Enter\\\" to allow the NIC software to be controlled remotely. This has been included by Neuroelectrics for security reasons, in order to avoid giving unauthorised access to the EEG Readings. Using the MatNICEEGRecord function (keeping TCP connection enabled), the CSV file is composed by as many columns as the channels of the device (in this case 20) and two more columns, one for the markers (if no markers have been sent, this column will be composed just by zeroes) and one for the timestamp. If instead the MatNICEEGRecordLSL function is used (through Lab Streaming Layer communication), the CSV file is composed by as many columns as the channels of the device (in this case 20). The units of the measurements for each channel are in nano-Volts (nV). Finally, I managed to synchronise the seventh game I developed with the EEG readings calling the MATLAB script I made, using Python. I therefore modified the code for the drawing canvas for two users by adding the matlab.engine library to call the MATLAB script file and using the threading library to make the canvas game and the EEG reading run simultaneously (without dividing the code into threads they would have run sequentially, compromising synchronisation). The main functions described above are available in Appendix B, Listings 1 and 2.","title":"EEG Cap"},{"location":"chapter4/","text":"Artificial Intelligence As part of this chapter, the analysis of an EEG data-set to identify if a child is affected or not by autism will be outlined. Both Machine Learning (ML) and Deep Learning (DL) approaches used to tackle this task will be examined. Additionally, a Computer Vision based approach to ASD screening will be proposed. Faces Responses in Children with ASD Children affected by ASD usually show difficulties in comprehending other people's emotions. A study carried out in 2012 by Fabio Apicella, Federico Sicca et al. (\"Fusiform Gyrus responses to neutral and emotional faces in children with Autism Spectrum Disorders: a High Density ERP study\" [32]) aimed to register the EEG response of a group of children (some of which were affected by ASD and some who weren't) to examine their reaction to different forms of stimuli (eg. Happy/Sad/Neutral faces, and images of Cartoons and Trees). Figure 4.1: Stimuli used. Image reproduced from: [32] Informed consent was given by all the children parent's before undertaking any measurement. Upon Ethics Approval, I made use of the EEG data (in the time-domain) collected during this study to determine whether it was possible to accurately classify which children were affected or not by ASD using Machine Learning. The data-set consisted of the EEG data (for all the five stimuli repeated multiple times) collected from twelve children affected by ASD and twelve who weren't. The data originally provided was in a MATLAB format. After a first exploratory analysis, I then decided to convert each file in a CSV format and then merged them so to perform my analysis using Python. I ended up making six CSV files, one for each of the five stimuli and one containing all the five stimuli together. For each child, the MATLAB data was stored using a three dimensional array. The first dimension represented the number of EEG channels (128), the second dimension represented the number of time-steps (250) and the third dimension the number of repetitions (between 20 and 80, depending on the child). Once the data had been converted in the CSV format, I structured it in a table. This table has 129 columns (one for each channel and one to label the data as either typical or ASD), every 250 rows was stored one stimulus repetition with their correspondent label. The data-set containing all the five stimulus, had been demonstrated to be balanced, containing 49.03% of the data of children without ASD and 50.97% of the data of ASD children. All the plots obtained during a first exploration of the data using MATLAB are present in Appendix C. The ML and Deep Leaning results obtained using the individual data-sets for each stimulus are instead present in Appendix D. ML Classification Once I constructed the full data-set containing all the five stimuli, I then decided to try to perform a classification task using Machine Learning. In order to do so, I made use of many Python libraries such as: pandas, numpy, matplotlib.pyplot, sklearn and itertools. The pre-processing part consisted of, firstly loading the data, standardising it and then dividing it in training (70%) and test sets (30%) for both the inputs (X) and outputs values (Y). The total number of rows in the data-set was equal to 1906500 (934750 rows about typical children EEG data and 971750 about ASD data). Because of the 70% against 30% train/test split ratio, 5338 predictions were made during the training set (1334550/250 time-steps) and 2288 predictions during the test set (571950/250 time-steps). I finally decided to train and then test the binary classification results using different algorithms such as: Logistic Regression, Support Vector Machines (SVM), Decision Trees, Linear Discriminant analysis (LDA) and Gaussian Naive Bayes Classifier (GNB). The classification results are show in Table 4.1. At the output, a zero represent a typical child and a one represents a child affected by ASD. The models used attempted to identify if a child is affected or not by ASD using just a single stimulus repetition (128 channels \u00d7 250 timesteps \u00d7 1 repetition). Table 4.1: ML Classification Accuracy The Decision Tree achieved the most accurate result. Decision Tree is a type of supervised learning algorithm which can be used for either classification or regression tasks. What distinguishes Decision Tree from other ML algorithms, is their ability to display their decision making process using an upside-down tree like representation. In the tree, each node represents a feature in the data-set, each branch a decision rule and each leaf a decision outcome. In this implementation, I decided to use the CART (Classification and Regression Trees) algorithm implementation which makes use of the Gini Index as metric (code available in Appendix B, Listing 3). Appendix E shows the beginning of the decision making process made by the Decision Tree to decide its classification criteria for this data-set. Performing PCA (to reduce data dimensionality to two dimensions) on the data lead to a sharp decrease in accuracy (51% accuracy at maximum using a Decision Tree). Figure 4.2: PCA Decision Tree Classification Results (0=Typical, 1=ASD) The trained Decision Tree model has then been successfully saved and stored (using the pickle library) to be ready for later use. Deep Learning Classification To increase accuracy of results, I then decided to implement Neural Networks (LSTM and CNN). I preferred to use these typologies of ANN instead of a Feed-Forward one because of their ability to work with sequential data. Most of the libraries used for ML Classification have been reused for the Deep Learning implementation, also the train/test ratio split has remained unaltered. Long Short-Term Memory In order to code this Neutral Network I decided to use the Google Open Source Deep Learning library Tensor-Flow. After careful literature background research, I decided to design the ANN architecture following the \"Human Activity Recognition using LSTMs on Android\" Medium guide [32]. For this implementation, I decided to change the pre-processing stage and some of the architecture parameters in order to best suit the data-set shape and characteristics (pre-processing code available in Appendix B, Listing 4) . Table 4.2: LSTM Parameters Letting training this model for 50 epochs, using the parameters shown in Table 4.2, lead to an impressive overall validation accuracy of 96.935 % (The full training accuracy and loss evolution is present in Appendix F). From Figure 4.3, it can be witnessed that both the training/test loss and the training/test accuracy curves closely match. This confirms the correct L2 regularisation (Ridge Regression) parameter was chosen. L2 regularisation aims to eliminate over-fitting by minimising the model complexity and loss. By using L2 regularisation, the model complexity is evaluated by summing and squaring all the model feature weights. One of the easier ways to identify overfitting in a model is by comparing the training and test sets loss functions. If the two are really similar, our model is successfully able to generalise to new data without losing accuracy. If instead the two functions are very dissimilar, our model might be able to perform well in the training phase but not during the test phase. Indicating that during the training our model overfitted the data learning also irrelevant information (such as noise) and was not able to correctly classify new data (Bias-Variance Trade-off). Figure 4.3: LSTM training/test loss and accuracy against epochs On the other side, the LSTM required a considerable amount of memory and time to train which could potentially become a problem if working with a greater amount of data. To avoid having to retrain the model, it was successfully saved and stored using the pickle library. Different metrics have been performed to test the overall efficiency of the model such as calculating the Confusion Matrix and AUC-ROC curve. Table 4.3: LSTM Confusion Matrix From the Confusion Matrix it is then possible to calculate the model Sensitivity and Specificity. Figure 4.4: LSTM ROC plot All these metrics confirmed that the model did not overfit the data as seen in Figure 4.3. In fact: The ROC curve score was close to 1 (0.997). The number of False Positives and False Negatives in the Confusion Matrix were close to each-other (therefore the model gave the same weight to both classes). Both the Sensitivity and Specificity results were higher than 97%. Convolutional Neural Network I finally decided to design a CNN in the hope to construct a faster model than the LSTM that would still able to achieve high accuracy. In order to construct the network architecture, I decided to use the Keras Python library. The model consisted of: One 2D Convolutional Layer of 64 filters, a kernel size of 5 \u00d7 5, a ReLU (Rectified Linear Unit, Equation 4.3) function and same padding. Another 2D Convolutional Layer having 32 filters, a kernel size of 5 \u00d7 5, a ReLU (rectified linear unit) function, same padding and an L2 regularisation coefficient of 0.01 (to prevent overfitting). A 2D MaxPooling layer of 2 \u00d7 2 size. A Dropout layer of 0.2 intensity (in order to avoid over-fitting the data). A layer to first flatten the data from three dimensions to just one, and then another one to condense the input to give to the classifier 128 features (always using the ReLU function and L2 regularisation). A second Dropout layer of 0.5 intensity. Finally, a Dense layer (of two neurons) to produce the classification result, using a Softmax activation function. The Softmax function (Equation 4.4) will take in this case the two inputs from the neurons and convert them in two probabilities that sums up to one. The greater probability was rounded up/down to either one or zero to represent the CNN output choice (Typical(0), ASD(1)). In order to optimise the training, the Adam (Adaptive Moment Estimation) gradient descent algorithm was used, and the cross-entropy function was used to compute the model loss. The cross-entropy function (H y' (y)), in a binary classification case can be calculated by using Equation 4.5. This model achieved an overall validation accuracy of 94.58% in just thirty epochs of training. Figure 4.5: CNN Model Training Both the LSTM and the CNN are able to classify if a child is affected or not by ASD just by observing one stimulus repetition (128 channels, 250 time-steps, 1 repetition, 1 stimulus). The overall accuracy of the CNN was 2% lower compared to the LSTM one, but the training time and amount of power needed to run this model was much lower. Also this model was successfully saved and stored (in a json and h5 file type) for later use (Code available in Appendix B, Listing 5). Also in this case different metrics have been performed to test the overall efficiency of the model such as calculating the Confusion Matrix and AUC-ROC Curve. Table 4.4: CNN Confusion Matrix From the Confusion Matrix it is then possible to calculate the model Sensitivity and Specificity. Figure 4.6: CNN ROC Curve For the same reasons examined in the LSTM analysis, the CNN model does not seem to be affected by over-fitting. However, the CNN performed overall worse than the LSTM. That's because of the bigger difference between the registered False Positives and False Negatives in the Confusion Matrix, which caused the Specificity score to be negatively affected. Autism Study Using Behaviour Imaging Computer Vision is an AI area which has seen exponential improvements during the last few years, especially because of the introduction of Convolutional Neural Networks. According to some research papers such as \"Computer vision and behavioural phenotyping: an autism case study\" [34] and \"Behaviour Imaging: Using Computer Vision to Study Autism\" [35], this subject can potentially have an huge impact in ASD detection in children. This can be done by: Making Computer Vision games to test child's ability to recognise and imitate facial expressions or feelings. Recording the child-therapist sessions to then examine the recording to find out if the child has shown: some reaction delay to the applied stimulus, any repetitive behaviour or any abnormal walking/speaking behaviour. Game Application As part of this project I decided to design a game to test a child's ability to make a series of facial expressions. In order to realise this game, I partially followed the \"Real Time Facial Expression Recognition on Streaming Data\" post by Sefik Ilkin Serengil [36] to make use of his pre-trained model for facial expression recognition. I successively coded the game using Python libraries such as: cv2, keras, tkinter and PIL. The game interface is formed by two windows. In the first one, the user is told if their face is correctly detected and what facial expression they should perform (Figure 4.7 (a)). In the second one, the camera recording is displayed and if a face is detected an empty rectangle will cover the face area displaying on the top left corner the detected expression (Figure 4.7 (b)). Once the user has made the asked expression, a pop-up window will appear to congratulate the user and the first window will update to ask to make a different face expression. The list of available face expressions consist of: angry, fear, happy, sad, surprise and neutral faces. Figure 4.7: Computer Vision Game","title":"Artificial Intelligence"},{"location":"chapter4/#artificial-intelligence","text":"As part of this chapter, the analysis of an EEG data-set to identify if a child is affected or not by autism will be outlined. Both Machine Learning (ML) and Deep Learning (DL) approaches used to tackle this task will be examined. Additionally, a Computer Vision based approach to ASD screening will be proposed.","title":"Artificial Intelligence"},{"location":"chapter4/#faces-responses-in-children-with-asd","text":"Children affected by ASD usually show difficulties in comprehending other people's emotions. A study carried out in 2012 by Fabio Apicella, Federico Sicca et al. (\"Fusiform Gyrus responses to neutral and emotional faces in children with Autism Spectrum Disorders: a High Density ERP study\" [32]) aimed to register the EEG response of a group of children (some of which were affected by ASD and some who weren't) to examine their reaction to different forms of stimuli (eg. Happy/Sad/Neutral faces, and images of Cartoons and Trees). Figure 4.1: Stimuli used. Image reproduced from: [32] Informed consent was given by all the children parent's before undertaking any measurement. Upon Ethics Approval, I made use of the EEG data (in the time-domain) collected during this study to determine whether it was possible to accurately classify which children were affected or not by ASD using Machine Learning. The data-set consisted of the EEG data (for all the five stimuli repeated multiple times) collected from twelve children affected by ASD and twelve who weren't. The data originally provided was in a MATLAB format. After a first exploratory analysis, I then decided to convert each file in a CSV format and then merged them so to perform my analysis using Python. I ended up making six CSV files, one for each of the five stimuli and one containing all the five stimuli together. For each child, the MATLAB data was stored using a three dimensional array. The first dimension represented the number of EEG channels (128), the second dimension represented the number of time-steps (250) and the third dimension the number of repetitions (between 20 and 80, depending on the child). Once the data had been converted in the CSV format, I structured it in a table. This table has 129 columns (one for each channel and one to label the data as either typical or ASD), every 250 rows was stored one stimulus repetition with their correspondent label. The data-set containing all the five stimulus, had been demonstrated to be balanced, containing 49.03% of the data of children without ASD and 50.97% of the data of ASD children. All the plots obtained during a first exploration of the data using MATLAB are present in Appendix C. The ML and Deep Leaning results obtained using the individual data-sets for each stimulus are instead present in Appendix D.","title":"Faces Responses in Children with ASD"},{"location":"chapter4/#ml-classification","text":"Once I constructed the full data-set containing all the five stimuli, I then decided to try to perform a classification task using Machine Learning. In order to do so, I made use of many Python libraries such as: pandas, numpy, matplotlib.pyplot, sklearn and itertools. The pre-processing part consisted of, firstly loading the data, standardising it and then dividing it in training (70%) and test sets (30%) for both the inputs (X) and outputs values (Y). The total number of rows in the data-set was equal to 1906500 (934750 rows about typical children EEG data and 971750 about ASD data). Because of the 70% against 30% train/test split ratio, 5338 predictions were made during the training set (1334550/250 time-steps) and 2288 predictions during the test set (571950/250 time-steps). I finally decided to train and then test the binary classification results using different algorithms such as: Logistic Regression, Support Vector Machines (SVM), Decision Trees, Linear Discriminant analysis (LDA) and Gaussian Naive Bayes Classifier (GNB). The classification results are show in Table 4.1. At the output, a zero represent a typical child and a one represents a child affected by ASD. The models used attempted to identify if a child is affected or not by ASD using just a single stimulus repetition (128 channels \u00d7 250 timesteps \u00d7 1 repetition). Table 4.1: ML Classification Accuracy The Decision Tree achieved the most accurate result. Decision Tree is a type of supervised learning algorithm which can be used for either classification or regression tasks. What distinguishes Decision Tree from other ML algorithms, is their ability to display their decision making process using an upside-down tree like representation. In the tree, each node represents a feature in the data-set, each branch a decision rule and each leaf a decision outcome. In this implementation, I decided to use the CART (Classification and Regression Trees) algorithm implementation which makes use of the Gini Index as metric (code available in Appendix B, Listing 3). Appendix E shows the beginning of the decision making process made by the Decision Tree to decide its classification criteria for this data-set. Performing PCA (to reduce data dimensionality to two dimensions) on the data lead to a sharp decrease in accuracy (51% accuracy at maximum using a Decision Tree). Figure 4.2: PCA Decision Tree Classification Results (0=Typical, 1=ASD) The trained Decision Tree model has then been successfully saved and stored (using the pickle library) to be ready for later use.","title":"ML Classification"},{"location":"chapter4/#deep-learning-classification","text":"To increase accuracy of results, I then decided to implement Neural Networks (LSTM and CNN). I preferred to use these typologies of ANN instead of a Feed-Forward one because of their ability to work with sequential data. Most of the libraries used for ML Classification have been reused for the Deep Learning implementation, also the train/test ratio split has remained unaltered.","title":"Deep Learning Classification"},{"location":"chapter4/#long-short-term-memory","text":"In order to code this Neutral Network I decided to use the Google Open Source Deep Learning library Tensor-Flow. After careful literature background research, I decided to design the ANN architecture following the \"Human Activity Recognition using LSTMs on Android\" Medium guide [32]. For this implementation, I decided to change the pre-processing stage and some of the architecture parameters in order to best suit the data-set shape and characteristics (pre-processing code available in Appendix B, Listing 4) . Table 4.2: LSTM Parameters Letting training this model for 50 epochs, using the parameters shown in Table 4.2, lead to an impressive overall validation accuracy of 96.935 % (The full training accuracy and loss evolution is present in Appendix F). From Figure 4.3, it can be witnessed that both the training/test loss and the training/test accuracy curves closely match. This confirms the correct L2 regularisation (Ridge Regression) parameter was chosen. L2 regularisation aims to eliminate over-fitting by minimising the model complexity and loss. By using L2 regularisation, the model complexity is evaluated by summing and squaring all the model feature weights. One of the easier ways to identify overfitting in a model is by comparing the training and test sets loss functions. If the two are really similar, our model is successfully able to generalise to new data without losing accuracy. If instead the two functions are very dissimilar, our model might be able to perform well in the training phase but not during the test phase. Indicating that during the training our model overfitted the data learning also irrelevant information (such as noise) and was not able to correctly classify new data (Bias-Variance Trade-off). Figure 4.3: LSTM training/test loss and accuracy against epochs On the other side, the LSTM required a considerable amount of memory and time to train which could potentially become a problem if working with a greater amount of data. To avoid having to retrain the model, it was successfully saved and stored using the pickle library. Different metrics have been performed to test the overall efficiency of the model such as calculating the Confusion Matrix and AUC-ROC curve. Table 4.3: LSTM Confusion Matrix From the Confusion Matrix it is then possible to calculate the model Sensitivity and Specificity. Figure 4.4: LSTM ROC plot All these metrics confirmed that the model did not overfit the data as seen in Figure 4.3. In fact: The ROC curve score was close to 1 (0.997). The number of False Positives and False Negatives in the Confusion Matrix were close to each-other (therefore the model gave the same weight to both classes). Both the Sensitivity and Specificity results were higher than 97%.","title":"Long Short-Term Memory"},{"location":"chapter4/#convolutional-neural-network","text":"I finally decided to design a CNN in the hope to construct a faster model than the LSTM that would still able to achieve high accuracy. In order to construct the network architecture, I decided to use the Keras Python library. The model consisted of: One 2D Convolutional Layer of 64 filters, a kernel size of 5 \u00d7 5, a ReLU (Rectified Linear Unit, Equation 4.3) function and same padding. Another 2D Convolutional Layer having 32 filters, a kernel size of 5 \u00d7 5, a ReLU (rectified linear unit) function, same padding and an L2 regularisation coefficient of 0.01 (to prevent overfitting). A 2D MaxPooling layer of 2 \u00d7 2 size. A Dropout layer of 0.2 intensity (in order to avoid over-fitting the data). A layer to first flatten the data from three dimensions to just one, and then another one to condense the input to give to the classifier 128 features (always using the ReLU function and L2 regularisation). A second Dropout layer of 0.5 intensity. Finally, a Dense layer (of two neurons) to produce the classification result, using a Softmax activation function. The Softmax function (Equation 4.4) will take in this case the two inputs from the neurons and convert them in two probabilities that sums up to one. The greater probability was rounded up/down to either one or zero to represent the CNN output choice (Typical(0), ASD(1)). In order to optimise the training, the Adam (Adaptive Moment Estimation) gradient descent algorithm was used, and the cross-entropy function was used to compute the model loss. The cross-entropy function (H y' (y)), in a binary classification case can be calculated by using Equation 4.5. This model achieved an overall validation accuracy of 94.58% in just thirty epochs of training. Figure 4.5: CNN Model Training Both the LSTM and the CNN are able to classify if a child is affected or not by ASD just by observing one stimulus repetition (128 channels, 250 time-steps, 1 repetition, 1 stimulus). The overall accuracy of the CNN was 2% lower compared to the LSTM one, but the training time and amount of power needed to run this model was much lower. Also this model was successfully saved and stored (in a json and h5 file type) for later use (Code available in Appendix B, Listing 5). Also in this case different metrics have been performed to test the overall efficiency of the model such as calculating the Confusion Matrix and AUC-ROC Curve. Table 4.4: CNN Confusion Matrix From the Confusion Matrix it is then possible to calculate the model Sensitivity and Specificity. Figure 4.6: CNN ROC Curve For the same reasons examined in the LSTM analysis, the CNN model does not seem to be affected by over-fitting. However, the CNN performed overall worse than the LSTM. That's because of the bigger difference between the registered False Positives and False Negatives in the Confusion Matrix, which caused the Specificity score to be negatively affected.","title":"Convolutional Neural Network"},{"location":"chapter4/#autism-study-using-behaviour-imaging","text":"Computer Vision is an AI area which has seen exponential improvements during the last few years, especially because of the introduction of Convolutional Neural Networks. According to some research papers such as \"Computer vision and behavioural phenotyping: an autism case study\" [34] and \"Behaviour Imaging: Using Computer Vision to Study Autism\" [35], this subject can potentially have an huge impact in ASD detection in children. This can be done by: Making Computer Vision games to test child's ability to recognise and imitate facial expressions or feelings. Recording the child-therapist sessions to then examine the recording to find out if the child has shown: some reaction delay to the applied stimulus, any repetitive behaviour or any abnormal walking/speaking behaviour.","title":"Autism Study Using Behaviour Imaging"},{"location":"chapter4/#game-application","text":"As part of this project I decided to design a game to test a child's ability to make a series of facial expressions. In order to realise this game, I partially followed the \"Real Time Facial Expression Recognition on Streaming Data\" post by Sefik Ilkin Serengil [36] to make use of his pre-trained model for facial expression recognition. I successively coded the game using Python libraries such as: cv2, keras, tkinter and PIL. The game interface is formed by two windows. In the first one, the user is told if their face is correctly detected and what facial expression they should perform (Figure 4.7 (a)). In the second one, the camera recording is displayed and if a face is detected an empty rectangle will cover the face area displaying on the top left corner the detected expression (Figure 4.7 (b)). Once the user has made the asked expression, a pop-up window will appear to congratulate the user and the first window will update to ask to make a different face expression. The list of available face expressions consist of: angry, fear, happy, sad, surprise and neutral faces. Figure 4.7: Computer Vision Game","title":"Game Application"},{"location":"chapter5/","text":"Conclusion Overall this project had a positive outcome and all the objectives prefixed have been established. Further Advancements Further developments can be envisaged especially concerning the Artificial Intelligence aspect of this project. The use of Wavelet analysis [37], Fast Fourier Transform [38], Entropy Analysis [39] or of an Auto-Encoder Network [40] during the pre-processing stage could potentially reduce the noise embedded in the registered signals therefore making classification easier. Additionally, it would also be interesting to convert the registered EEG signals from the time domain to the frequency domain to see if that could improve the neural networks performances. Another issue to be taken into account could be the reliability of the data used (eg. if the data was collected correctly, if all the noise registered during the measurements had been correctly filtered out...) and how reproducible these accuracy results can be using other forms of data which have been collected using a different types of EEG Cap or under different laboratory conditions. When trying to use Artificial Intelligence in medical applications, high accuracy and reliability must be obtained. Explainable AI (creating models able to explain himself their classification decision making process) could help doctors to understand how/if to use AI when making decisions.","title":"Conclusion"},{"location":"chapter5/#conclusion","text":"Overall this project had a positive outcome and all the objectives prefixed have been established.","title":"Conclusion"},{"location":"chapter5/#further-advancements","text":"Further developments can be envisaged especially concerning the Artificial Intelligence aspect of this project. The use of Wavelet analysis [37], Fast Fourier Transform [38], Entropy Analysis [39] or of an Auto-Encoder Network [40] during the pre-processing stage could potentially reduce the noise embedded in the registered signals therefore making classification easier. Additionally, it would also be interesting to convert the registered EEG signals from the time domain to the frequency domain to see if that could improve the neural networks performances. Another issue to be taken into account could be the reliability of the data used (eg. if the data was collected correctly, if all the noise registered during the measurements had been correctly filtered out...) and how reproducible these accuracy results can be using other forms of data which have been collected using a different types of EEG Cap or under different laboratory conditions. When trying to use Artificial Intelligence in medical applications, high accuracy and reliability must be obtained. Explainable AI (creating models able to explain himself their classification decision making process) could help doctors to understand how/if to use AI when making decisions.","title":"Further Advancements"},{"location":"chapter6/","text":"Bibliography [1] Division of Birth Defects, National Center on Birth Defects and Developmental Disabilities, Centers for Disease Control and Prevention. Accessed: https://www.cdc.gov/ncbddd/autism/data.html December 2018. [2] Giacomo Vivanti, Cheryl Dissanayake, Cynthia Zierhut, Sally J. Rogers, Victorian ASELCC Team Brief Report: Predictors of Outcomes in the Early Start Denver Model Delivered in a Group Setting. Accessed: https://link.springer.com/article/10.1007/s10803-012-1705-7 December 2018. [3] Susan E Bryson, Lonnie Zwaigenbaum, Wendy Roberts The early detection of autism in clinical practice. Accessed: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2720498/pdf/pch09219.pdf March 2019. [4] Alabama Phycological Services Center, LLC AUTISM SPECTRUM DISORDER. Accessed: http://alapsych.com/autism-spectrum-disorder/ March 2019. [5] Q-CHAT Quantitative Checklist for Autism in Toddlers. Accessed: https://psychology-tools.com/test/qchat-quantitative-checklist-for-autism-in-toddlers March 2019. [6] Tristram Smith, Svein Eikeseth. O. Ivar Lovaas: Pioneer of Applied Behavior Analysis and Intervention for Children with Autism. Accessed: https://link.springer.com/article/10.1007/s10803-010-1162-0 December 2018. [7] Corinna F. Grindle, Hanna Kovshoff, Richard P. Hastings, Bob Remington. Parents' Experiences of Home-Based Applied Behavior Analysis Programs for Young Children with Autism. Accessed: https://link.springer.com/article/10.1007/s10803-008-0597-z December 2018. [8] Geraldine Dawson, Sally Rogers, Jeffrey Munson, Milani Smith, Jamie Winter, Jessica Greenson, Amy Donaldson, Jennifer Varley. Randomized, Controlled Trial of an Intervention for Toddlers With Autism: The Early Start Denver Model. Accessed: http://pediatrics.aappublications.org/content/125/1/e17.short December 2018. [9] Brainworks, train your mind. What are brainwaves?. Accessed: https://brainworksneurotherapy.com/what-are-brainwaves December 2018. [10] FONSECA, Lineu C. et al. Quantitative EEG in children with learning disabilities: analysis of band power. Accessed: https://www.ncbi.nlm.nih.gov/pubmed/16917604 December 2018. [11] What are serious games? - Growth Engineering. Accessed: https://www.growthengineering.co.uk/what-are-serious-games/ February 2019. [12] Gamification Versus Serious Games - Andrew Hughes - Training Industry. Accessed: https://trainingindustry.com/articles/learning-technologies/gamification-versus-serious-games/ February 2019. [13] Fedwa Laamarti, Mohamad Eid, and Abdulmotaleb El Saddik, \\\"An Overview of Serious Games\\\", International Journal of Computer Games Technology, vol. 2014, Article ID 358152, 15 pages, 2014. Accessed: https://www.hindawi.com/journals/ijcgt/2014/358152/ February 2019. [14] History of AI - Shaan Ray - Towards Data Science. Accessed: https://towardsdatascience.com/history-of-ai-484a86fc16ef January 2019. [15] Artificial Intelligence: Definition, Types, Examples, Technologies - Chethan Kumar GN - Medium. Accessed: https://medium.com/@chethankumargn/artificial-intelligence-definition-types-examples-technologies-962ea75c7b9b January 2019. [16] Cousins of Artificial Intelligence - Seema Singh - Towards Data Science. Accessed: https://towardsdatascience.com/cousins-of-artificial-intelligence-dda4edc27b55 January 2019. [17] Understanding LSTM Networks, colah's blog Accessed: http://colah.github.io/posts/2015-08-Understanding-LSTMs/ March 2019 [18] Understanding LSTM and its diagrams, Shi Yan Accessed: https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714 March 2019. [19] An intuitive guide to Convolutional Neural Networks, Daphne Cornelisse Accessed: https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050 March 2019. [20] Max-pooling / Pooling, computersciencewiki.org Accessed: https://computersciencewiki.org/index.php/Max-pooling_/_Pooling March 2019. [21] Understanding AUC - ROC Curve, Sarang Narkhede, Medium Accessed: https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5 March 2019. [22] Choosing the Right Metric for Evaluating Machine Learning Models\u200a---\u200aPart 2, Alvira Swalin, University of San Francisco, KDnuggets Accessed: https://www.kdnuggets.com/2018/06/right-metric-evaluating-machine-learning-models-2.html March 2019. [23] Artificial Intelligence at Google - Our Principles - Google AI. Accessed: https://ai.google/principles/ January 2019. [24] Wikipedia, the free encyclopedia. Wii Remote. Accessed: https://en.wikipedia.org/wiki/Wii_Remote November 2018. [25] Frederick Vandenbosch DLP LightCrafter Display 2000 EVM on Raspberry Pi. Accessed: http://frederickvandenbosch.be/?p=2948 November 2018. [26] Martin Rudel python-whiteboard. Accessed: https://github.com/pnegre/python-whiteboard/wiki November 2018. [27] Deck.Toys Smoothboard Air with Duo. Accessed: https://en.freedownloadmanager.org/Windows-PC/SmoothboardAirwithDuo.html November 2018. [28] Pygame Game Development. Accesed: https://pythonprogramming.net November 2018. [29] McGugan, Will Beginning Game Development with Python and Pygame: From Novice to Professional (Beginning from Novice to Professional) 2007, ISBN = 1590598725, Apress. [30] Neuroelectronics Enobio 20. Accessed: https://www.neuroelectrics.com/products/enobio/enobio-20 November 2018. [31] Neuroelectronics MatNIC Toolkit. Accessed: https://www.neuroelectrics.com/products/software/matnic-remote-stimulation-client November 2018. [32] Fabio Apicella, Federico Sicca et al. Fusiform Gyrus responses to neutral and emotional faces in children with Autism Spectrum Disorders: a High Density ERP study. Accessed: https://www.ncbi.nlm.nih.gov/pubmed/23124137 March 2019. [33] Venelin Valkov - Medium Human Activity Recognition using LSTMs on Android --- TensorFlow for Hackers (Part VI). Accessed: March 2019. [34] James M. Rehg, Center for Behavior Imaging, Georgia Institute of Technology Behavior Imaging: Using Computer Vision to Study Autism. Accessed: https://pdfs.semanticscholar.org/88e2/be0d767e0a4d077a96a006e2554653d709ed.pdf March 2019. [35] Guillermo Sapiro, Jordan Hashemi and Geraldine Dawson. ScienceDirect Computer vision and behavioral phenotyping: an autism case study. Accessed: https://www.sciencedirect.com/science/article/pii/S246845111830059X March 2019. [36] Sefik Ilkin Serengil, Developer's Log Real Time Facial Expression Recognition on Streaming Data. Accessed: https://sefiks.com/2018/01/10/real-time-facial-expression-recognition-on-streaming-data/ March 2019. [37] Neep Hazarika, Jean Zhu Chen, Ah Chung Tsoi, Alex Sergejew - ScienceDirect Classification of EEG signals using the wavelet transform. Accessed: https://www.sciencedirect.com/science/article/pii/S0165168497000388 March 2019. [38] Maan M. Shaker - International Journal of Medical, Health, Biomedical, Bioengineering and Pharmaceutical Engineering Vol:1, No:3, 2007 EEG Waves Classifier using Wavelet Transform and Fourier Transform. Accessed: https://pdfs.semanticscholar.org/6b88/8f863dca3e43be157189d18a7441cf802b98.pdf March 2019. [39] Peng Li, Chandan Karmakar, et al. - Plos|One Detection of epileptic seizure based on entropy analysis of short-term EEG. Accessed: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0193691 March 2019. [40] Jeremy Jordan Introduction to autoencoders. Accessed: https://www.jeremyjordan.me/autoencoders/ March 2019.","title":"Bibliography"},{"location":"chapter6/#bibliography","text":"[1] Division of Birth Defects, National Center on Birth Defects and Developmental Disabilities, Centers for Disease Control and Prevention. Accessed: https://www.cdc.gov/ncbddd/autism/data.html December 2018. [2] Giacomo Vivanti, Cheryl Dissanayake, Cynthia Zierhut, Sally J. Rogers, Victorian ASELCC Team Brief Report: Predictors of Outcomes in the Early Start Denver Model Delivered in a Group Setting. Accessed: https://link.springer.com/article/10.1007/s10803-012-1705-7 December 2018. [3] Susan E Bryson, Lonnie Zwaigenbaum, Wendy Roberts The early detection of autism in clinical practice. Accessed: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC2720498/pdf/pch09219.pdf March 2019. [4] Alabama Phycological Services Center, LLC AUTISM SPECTRUM DISORDER. Accessed: http://alapsych.com/autism-spectrum-disorder/ March 2019. [5] Q-CHAT Quantitative Checklist for Autism in Toddlers. Accessed: https://psychology-tools.com/test/qchat-quantitative-checklist-for-autism-in-toddlers March 2019. [6] Tristram Smith, Svein Eikeseth. O. Ivar Lovaas: Pioneer of Applied Behavior Analysis and Intervention for Children with Autism. Accessed: https://link.springer.com/article/10.1007/s10803-010-1162-0 December 2018. [7] Corinna F. Grindle, Hanna Kovshoff, Richard P. Hastings, Bob Remington. Parents' Experiences of Home-Based Applied Behavior Analysis Programs for Young Children with Autism. Accessed: https://link.springer.com/article/10.1007/s10803-008-0597-z December 2018. [8] Geraldine Dawson, Sally Rogers, Jeffrey Munson, Milani Smith, Jamie Winter, Jessica Greenson, Amy Donaldson, Jennifer Varley. Randomized, Controlled Trial of an Intervention for Toddlers With Autism: The Early Start Denver Model. Accessed: http://pediatrics.aappublications.org/content/125/1/e17.short December 2018. [9] Brainworks, train your mind. What are brainwaves?. Accessed: https://brainworksneurotherapy.com/what-are-brainwaves December 2018. [10] FONSECA, Lineu C. et al. Quantitative EEG in children with learning disabilities: analysis of band power. Accessed: https://www.ncbi.nlm.nih.gov/pubmed/16917604 December 2018. [11] What are serious games? - Growth Engineering. Accessed: https://www.growthengineering.co.uk/what-are-serious-games/ February 2019. [12] Gamification Versus Serious Games - Andrew Hughes - Training Industry. Accessed: https://trainingindustry.com/articles/learning-technologies/gamification-versus-serious-games/ February 2019. [13] Fedwa Laamarti, Mohamad Eid, and Abdulmotaleb El Saddik, \\\"An Overview of Serious Games\\\", International Journal of Computer Games Technology, vol. 2014, Article ID 358152, 15 pages, 2014. Accessed: https://www.hindawi.com/journals/ijcgt/2014/358152/ February 2019. [14] History of AI - Shaan Ray - Towards Data Science. Accessed: https://towardsdatascience.com/history-of-ai-484a86fc16ef January 2019. [15] Artificial Intelligence: Definition, Types, Examples, Technologies - Chethan Kumar GN - Medium. Accessed: https://medium.com/@chethankumargn/artificial-intelligence-definition-types-examples-technologies-962ea75c7b9b January 2019. [16] Cousins of Artificial Intelligence - Seema Singh - Towards Data Science. Accessed: https://towardsdatascience.com/cousins-of-artificial-intelligence-dda4edc27b55 January 2019. [17] Understanding LSTM Networks, colah's blog Accessed: http://colah.github.io/posts/2015-08-Understanding-LSTMs/ March 2019 [18] Understanding LSTM and its diagrams, Shi Yan Accessed: https://medium.com/mlreview/understanding-lstm-and-its-diagrams-37e2f46f1714 March 2019. [19] An intuitive guide to Convolutional Neural Networks, Daphne Cornelisse Accessed: https://medium.freecodecamp.org/an-intuitive-guide-to-convolutional-neural-networks-260c2de0a050 March 2019. [20] Max-pooling / Pooling, computersciencewiki.org Accessed: https://computersciencewiki.org/index.php/Max-pooling_/_Pooling March 2019. [21] Understanding AUC - ROC Curve, Sarang Narkhede, Medium Accessed: https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5 March 2019. [22] Choosing the Right Metric for Evaluating Machine Learning Models\u200a---\u200aPart 2, Alvira Swalin, University of San Francisco, KDnuggets Accessed: https://www.kdnuggets.com/2018/06/right-metric-evaluating-machine-learning-models-2.html March 2019. [23] Artificial Intelligence at Google - Our Principles - Google AI. Accessed: https://ai.google/principles/ January 2019. [24] Wikipedia, the free encyclopedia. Wii Remote. Accessed: https://en.wikipedia.org/wiki/Wii_Remote November 2018. [25] Frederick Vandenbosch DLP LightCrafter Display 2000 EVM on Raspberry Pi. Accessed: http://frederickvandenbosch.be/?p=2948 November 2018. [26] Martin Rudel python-whiteboard. Accessed: https://github.com/pnegre/python-whiteboard/wiki November 2018. [27] Deck.Toys Smoothboard Air with Duo. Accessed: https://en.freedownloadmanager.org/Windows-PC/SmoothboardAirwithDuo.html November 2018. [28] Pygame Game Development. Accesed: https://pythonprogramming.net November 2018. [29] McGugan, Will Beginning Game Development with Python and Pygame: From Novice to Professional (Beginning from Novice to Professional) 2007, ISBN = 1590598725, Apress. [30] Neuroelectronics Enobio 20. Accessed: https://www.neuroelectrics.com/products/enobio/enobio-20 November 2018. [31] Neuroelectronics MatNIC Toolkit. Accessed: https://www.neuroelectrics.com/products/software/matnic-remote-stimulation-client November 2018. [32] Fabio Apicella, Federico Sicca et al. Fusiform Gyrus responses to neutral and emotional faces in children with Autism Spectrum Disorders: a High Density ERP study. Accessed: https://www.ncbi.nlm.nih.gov/pubmed/23124137 March 2019. [33] Venelin Valkov - Medium Human Activity Recognition using LSTMs on Android --- TensorFlow for Hackers (Part VI). Accessed: March 2019. [34] James M. Rehg, Center for Behavior Imaging, Georgia Institute of Technology Behavior Imaging: Using Computer Vision to Study Autism. Accessed: https://pdfs.semanticscholar.org/88e2/be0d767e0a4d077a96a006e2554653d709ed.pdf March 2019. [35] Guillermo Sapiro, Jordan Hashemi and Geraldine Dawson. ScienceDirect Computer vision and behavioral phenotyping: an autism case study. Accessed: https://www.sciencedirect.com/science/article/pii/S246845111830059X March 2019. [36] Sefik Ilkin Serengil, Developer's Log Real Time Facial Expression Recognition on Streaming Data. Accessed: https://sefiks.com/2018/01/10/real-time-facial-expression-recognition-on-streaming-data/ March 2019. [37] Neep Hazarika, Jean Zhu Chen, Ah Chung Tsoi, Alex Sergejew - ScienceDirect Classification of EEG signals using the wavelet transform. Accessed: https://www.sciencedirect.com/science/article/pii/S0165168497000388 March 2019. [38] Maan M. Shaker - International Journal of Medical, Health, Biomedical, Bioengineering and Pharmaceutical Engineering Vol:1, No:3, 2007 EEG Waves Classifier using Wavelet Transform and Fourier Transform. Accessed: https://pdfs.semanticscholar.org/6b88/8f863dca3e43be157189d18a7441cf802b98.pdf March 2019. [39] Peng Li, Chandan Karmakar, et al. - Plos|One Detection of epileptic seizure based on entropy analysis of short-term EEG. Accessed: https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0193691 March 2019. [40] Jeremy Jordan Introduction to autoencoders. Accessed: https://www.jeremyjordan.me/autoencoders/ March 2019.","title":"Bibliography"},{"location":"chapter7/","text":"Appendices Appendix A: Evaluation Board wiring and stand Figure A.1: Evaluation Board Figure A.2: Raspberry Pi's GPIO Images reproduced from Frederick Vandenbosch DLP, LightCrafter Display 2000 EVM on Raspberry Pi [25]. Figure A.3: Projector stand design made using Fusion 360 Appendix B: Listings In Listing 1 has been chosen a period of 30 seconds and 20 channels. [ret, intletEEG] = MatNICEEGConnectLSL('NIC') [ret, eeg_set, timestamp_set] = MatNICEEGRecordLSL(30, 20,intletEEG) csvwrite('output-gameplay.csv',eeg_set) Listing 1: MATLAB code setupscript3.m import matlab.engine from threading import Thread # EEG readings pre-game and set-up eng = matlab.engine.start_matlab() eng.addpath(r'path_to_file', nargout=0) tf = eng.setupscript(nargout=0) print(tf) # Defining threading function to get and store eeg data while playing def eeg_func(): # EEG readings during-game eng = matlab.engine.start_matlab() eng.addpath(r'path_to_file', nargout=0) tf3 = eng.setupscript3(nargout=0) print(tf3) Thread(target = eeg_func).start() Listing 2: Python code added to enable EEG readings synchronisation trainedtree = tree.DecisionTreeClassifier().fit(X_Train, Y_Train) predictionstree = trainedtree.predict(X_Test) print(confusion_matrix(Y_Test,predictionstree)) print(classification_report(Y_Test,predictionstree)) Listing 3: Decision Tree Code segments = [] for i in range(0, len(df) - N_TIME_STEPS, step): ch = [] for j in range(0, N_FEATURES): ch.append(df.iloc[:, j].values[i: i + N_TIME_STEPS]) segments.append(ch) labels = [] for i in range(0, len(df) - N_TIME_STEPS, step): label = stats.mode(df['Label'][i: i + N_TIME_STEPS])[0][0] labels.append(label) labelsl = np.asarray(pd.get_dummies(labels), dtype = np.float32) reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, N_TIME_STEPS, N_FEATURES) X_train, X_test, y_train, y_test = train_test_split( reshaped_segments, labelsl, test_size=0.2, random_state=RANDOM_SEED) Listing 4: LSTM Pre-processing model_json = model.to_json() with open(\"model.json\", \"w\") as json_file: json_file.write(model_json) model.save_weights(\"model.h5\") Listing 5: Storing CNN Model Appendix C: ML MATLAB Analysis Plot of all the channels voltages registered for all the time-steps (repetition number one) in Happy Data, Typical Child number 1. Figure C.1: EEG voltage across all the channels, all time-steps (Repetition 1), Happy Data, TYP1 Plot of all the channels voltages registered for all the time-steps (repetition number one) in Happy Data, ASD Child number 1. Figure C.2: EEG voltage across all the channels, all time-steps (Repetition 1), Happy Data, ASD1 Plots of the channel number against the voltage amplitude of EEG brainwaves for the first typical child (on the left side) and the first ASD child (on the right side), this is done for all the 5 stimuli and for their corresponded experiment repetitions number 1 (left side of each plot) and 35 (right side of each plot). All the channels, all the time-steps and just one repetition is considered for each graph. Figure C.3: Happy Data Stimulus Figure C.4: Neutral Data Stimulus Figure C.5: Fear Data Stimulus Figure C.6: Tree Data Stimulus Figure C.7: Cartoon Data Stimulus Considering just channel number 1, all the time-steps and just one repetition (Repetition number 1 on the left and repetition number 35 on the right of each plot), the following results can be obtained: Figure C.8: Happy Data Stimulus Figure C.9: Neutral Data Stimulus Figure C.10: Fear Data Stimulus Figure C.11: Tree Data Stimulus Figure C.12: Cartoon Data Stimulus Appendix D: Individual Stimulus Data-sets Results Table 1: Decision Tree accuracy for Individual Stimulus Data-sets For the LSTM because of the reduced amount of data when working with the individual stimulus, has been chosen a Train/Test split ratio of 80% against 20%. Table 2: LSTM Parameters for Individual Stimulus Data-sets Table 3: LSTM accuracy for Individual Stimulus Data-sets For the CNN has instead been kept a Train/Test split ratio of 70% against 30% and the model architecture has been slightly modified to best fit the reduced amount of data: Two 2D Convolutional Layers having 32 filters, a kernel size of 5 \u00d7 5, a ReLU (rectified linear unit) function and the same padding. A 2D MaxPooling layer of 2 \u00d7 2 size. A Dropout layer of 0.2 intensity (in order to avoid over-fitting the data) A layer to first flatten the data from three dimensions to just one, and then another one to condense the input to give to the classifier 128 features (always using the ReLU function). A second Dropout layer of 0.5 intensity. Finally, a Dense layer (of two neurons) to produce the classification result, using a Softmax activation function. Table 4: CNN accuracy for Individual Stimulus Data-sets Figure D.1: PCA classification using Decision Tree Appendix E: Decision Tree Classification The following graph was realised storing the tree as a .dot file and then using the Graphviz library. For the purposes of space, just the first 48 branches are displayed because the full decision tree is composed by more than 9000 branches. Figure E.1: Decision Tree Classification Plot Appendix F: LSTM Training/Test Loss and Accuracy Figure F.1: LSTM Training/Test Loss and Accuracy","title":"Appendices"},{"location":"chapter7/#appendices","text":"","title":"Appendices"},{"location":"chapter7/#appendix-a-evaluation-board-wiring-and-stand","text":"Figure A.1: Evaluation Board Figure A.2: Raspberry Pi's GPIO Images reproduced from Frederick Vandenbosch DLP, LightCrafter Display 2000 EVM on Raspberry Pi [25]. Figure A.3: Projector stand design made using Fusion 360","title":"Appendix A: Evaluation Board wiring and stand"},{"location":"chapter7/#appendix-b-listings","text":"In Listing 1 has been chosen a period of 30 seconds and 20 channels. [ret, intletEEG] = MatNICEEGConnectLSL('NIC') [ret, eeg_set, timestamp_set] = MatNICEEGRecordLSL(30, 20,intletEEG) csvwrite('output-gameplay.csv',eeg_set) Listing 1: MATLAB code setupscript3.m import matlab.engine from threading import Thread # EEG readings pre-game and set-up eng = matlab.engine.start_matlab() eng.addpath(r'path_to_file', nargout=0) tf = eng.setupscript(nargout=0) print(tf) # Defining threading function to get and store eeg data while playing def eeg_func(): # EEG readings during-game eng = matlab.engine.start_matlab() eng.addpath(r'path_to_file', nargout=0) tf3 = eng.setupscript3(nargout=0) print(tf3) Thread(target = eeg_func).start() Listing 2: Python code added to enable EEG readings synchronisation trainedtree = tree.DecisionTreeClassifier().fit(X_Train, Y_Train) predictionstree = trainedtree.predict(X_Test) print(confusion_matrix(Y_Test,predictionstree)) print(classification_report(Y_Test,predictionstree)) Listing 3: Decision Tree Code segments = [] for i in range(0, len(df) - N_TIME_STEPS, step): ch = [] for j in range(0, N_FEATURES): ch.append(df.iloc[:, j].values[i: i + N_TIME_STEPS]) segments.append(ch) labels = [] for i in range(0, len(df) - N_TIME_STEPS, step): label = stats.mode(df['Label'][i: i + N_TIME_STEPS])[0][0] labels.append(label) labelsl = np.asarray(pd.get_dummies(labels), dtype = np.float32) reshaped_segments = np.asarray(segments, dtype= np.float32).reshape(-1, N_TIME_STEPS, N_FEATURES) X_train, X_test, y_train, y_test = train_test_split( reshaped_segments, labelsl, test_size=0.2, random_state=RANDOM_SEED) Listing 4: LSTM Pre-processing model_json = model.to_json() with open(\"model.json\", \"w\") as json_file: json_file.write(model_json) model.save_weights(\"model.h5\") Listing 5: Storing CNN Model","title":"Appendix B: Listings"},{"location":"chapter7/#appendix-c-ml-matlab-analysis","text":"Plot of all the channels voltages registered for all the time-steps (repetition number one) in Happy Data, Typical Child number 1. Figure C.1: EEG voltage across all the channels, all time-steps (Repetition 1), Happy Data, TYP1 Plot of all the channels voltages registered for all the time-steps (repetition number one) in Happy Data, ASD Child number 1. Figure C.2: EEG voltage across all the channels, all time-steps (Repetition 1), Happy Data, ASD1 Plots of the channel number against the voltage amplitude of EEG brainwaves for the first typical child (on the left side) and the first ASD child (on the right side), this is done for all the 5 stimuli and for their corresponded experiment repetitions number 1 (left side of each plot) and 35 (right side of each plot). All the channels, all the time-steps and just one repetition is considered for each graph. Figure C.3: Happy Data Stimulus Figure C.4: Neutral Data Stimulus Figure C.5: Fear Data Stimulus Figure C.6: Tree Data Stimulus Figure C.7: Cartoon Data Stimulus Considering just channel number 1, all the time-steps and just one repetition (Repetition number 1 on the left and repetition number 35 on the right of each plot), the following results can be obtained: Figure C.8: Happy Data Stimulus Figure C.9: Neutral Data Stimulus Figure C.10: Fear Data Stimulus Figure C.11: Tree Data Stimulus Figure C.12: Cartoon Data Stimulus","title":"Appendix C: ML MATLAB Analysis"},{"location":"chapter7/#appendix-d-individual-stimulus-data-sets-results","text":"Table 1: Decision Tree accuracy for Individual Stimulus Data-sets For the LSTM because of the reduced amount of data when working with the individual stimulus, has been chosen a Train/Test split ratio of 80% against 20%. Table 2: LSTM Parameters for Individual Stimulus Data-sets Table 3: LSTM accuracy for Individual Stimulus Data-sets For the CNN has instead been kept a Train/Test split ratio of 70% against 30% and the model architecture has been slightly modified to best fit the reduced amount of data: Two 2D Convolutional Layers having 32 filters, a kernel size of 5 \u00d7 5, a ReLU (rectified linear unit) function and the same padding. A 2D MaxPooling layer of 2 \u00d7 2 size. A Dropout layer of 0.2 intensity (in order to avoid over-fitting the data) A layer to first flatten the data from three dimensions to just one, and then another one to condense the input to give to the classifier 128 features (always using the ReLU function). A second Dropout layer of 0.5 intensity. Finally, a Dense layer (of two neurons) to produce the classification result, using a Softmax activation function. Table 4: CNN accuracy for Individual Stimulus Data-sets Figure D.1: PCA classification using Decision Tree","title":"Appendix D: Individual Stimulus Data-sets Results"},{"location":"chapter7/#appendix-e-decision-tree-classification","text":"The following graph was realised storing the tree as a .dot file and then using the Graphviz library. For the purposes of space, just the first 48 branches are displayed because the full decision tree is composed by more than 9000 branches. Figure E.1: Decision Tree Classification Plot","title":"Appendix E: Decision Tree Classification"},{"location":"chapter7/#appendix-f-lstm-trainingtest-loss-and-accuracy","text":"Figure F.1: LSTM Training/Test Loss and Accuracy","title":"Appendix F: LSTM Training/Test Loss and Accuracy"}]}